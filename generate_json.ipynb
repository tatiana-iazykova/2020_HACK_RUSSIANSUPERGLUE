{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "generate_json.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "IwtBjRQwOEhE",
        "JxRpQnEwRB3V",
        "CNQmcII0z1Ej",
        "M12zdZwykq5G",
        "Tt16TWzM5bR3",
        "LlsZGLAu6U1C",
        "vp4MWtzPOIUq",
        "HmTCOY9tSLHd",
        "1nn08m3ATJMW",
        "s4n1h3Kje-1O",
        "6whDZkEsf2Bb",
        "Wq1eLrv8dwye"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tatiana-iazykova/2020_HACK_RUSSIANSUPERGLUE/blob/main/generate_json.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7lPbpWjqF3t_",
        "outputId": "abcd8359-ef64-4795-f367-bba1bc048335"
      },
      "source": [
        "!wget -q --show-progress \"https://raw.githubusercontent.com/tatiana-iazykova/2020_HACK_RUSSIANSUPERGLUE/main/base.py\" -O base.py"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\rbase.py               0%[                    ]       0  --.-KB/s               \rbase.py             100%[===================>]   3.44K  --.-KB/s    in 0s      \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1f5wl93tIuLt"
      },
      "source": [
        "%%capture\r\n",
        "!wget https://russiansuperglue.com/tasks/download\r\n",
        "!unzip download\r\n",
        "!rm download\r\n",
        "!rm -r /content/__MACOSX\r\n",
        "!rm -r sample_data/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q9vbghmVGEJE"
      },
      "source": [
        "from pathlib import Path\n",
        "data_dir = Path(\"combined/\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m9h5aNHOJu4p"
      },
      "source": [
        "import pandas as pd\r\n",
        "\r\n",
        "class JSONL_handler():\r\n",
        "    \"\"\" opens a jsonl file and turns it into a necessary data structure \"\"\"\r\n",
        "    \r\n",
        "    def __init__(self, path):\r\n",
        "        self.path = path # path to jsonl file\r\n",
        "\r\n",
        "    def to_pandas(self):\r\n",
        "        \"\"\" get jsonl file content as a pandas DataFrame\"\"\"\r\n",
        "        return pd.read_json(path_or_buf=self.path, lines=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IwtBjRQwOEhE"
      },
      "source": [
        "# Baseline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oeikZ_jjGN2g"
      },
      "source": [
        "output_dir = Path(\"random_submission\")\n",
        "!mkdir $output_dir"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MiJ5Xvq7N9g5"
      },
      "source": [
        "output_dir_majority = Path(\"majority_submission\")\r\n",
        "!mkdir $output_dir_majority"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hLUxyRPbOBQS"
      },
      "source": [
        "output_dir_random_weighted = Path(\"random_weighted_submission\")\r\n",
        "!mkdir $output_dir_random_weighted"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l7wP1Ix-LIjC"
      },
      "source": [
        "from base import BaseSolverSubmit\r\n",
        "import json\r\n",
        "\r\n",
        "class Random_submission():\r\n",
        "  def __init__(self, dataset, path = None, path_valid = None, path_test = None):\r\n",
        "    self.dataset = dataset\r\n",
        "    self.path = '/content/combined/' + dataset + '/train.jsonl' if path is None else path\r\n",
        "    self.path_valid = '/content/combined/' + dataset + '/val.jsonl' if path_valid is None else path_valid\r\n",
        "    self.path_test = '/content/combined/' + dataset + '/test.jsonl' if path_test is None else path_test\r\n",
        "\r\n",
        "  def test_output(self):\r\n",
        "    test = JSONL_handler(self.path_test).to_pandas()\r\n",
        "    test_pred = [{\"idx\": idx, \"label\": str(label).lower()} for idx, label in zip(test.idx, self.scores)]\r\n",
        "    return test_pred\r\n",
        "\r\n",
        "  def get_scores_random(self):\r\n",
        "    solver = BaseSolverSubmit(path = self.path, path_valid = self.path_valid, path_test = self.path_test)\r\n",
        "    self.scores = solver.random_choice(len(solver.valid))\r\n",
        "    filename = self.dataset + \".jsonl\"\r\n",
        "    self.save_output(self.test_output(), output_dir / filename)\r\n",
        "  \r\n",
        "  def get_scores_majority(self):\r\n",
        "    solver = BaseSolverSubmit(path = self.path, path_valid = self.path_valid, path_test = self.path_test)\r\n",
        "    self.scores = solver.majority_class(len(solver.valid))\r\n",
        "    filename = self.dataset + \".jsonl\"\r\n",
        "    self.save_output(self.test_output(), output_dir_majority / filename)\r\n",
        "\r\n",
        "  def get_scores_random_weighted(self):\r\n",
        "    solver = BaseSolverSubmit(path = self.path, path_valid = self.path_valid, path_test = self.path_test)\r\n",
        "    self.scores = solver.random_balanced_choice(len(solver.valid))\r\n",
        "    filename = self.dataset + \".jsonl\"\r\n",
        "    self.save_output(self.test_output(), output_dir_random_weighted / filename)\r\n",
        "  \r\n",
        "  def save_output(self, data, path):\r\n",
        "    with open(path, mode=\"w\") as file:\r\n",
        "        for line in sorted(data, key=lambda x: int(x.get(\"idx\"))):\r\n",
        "            line[\"idx\"] = int(line[\"idx\"])\r\n",
        "            file.write(f\"{json.dumps(line, ensure_ascii=False)}\\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JxRpQnEwRB3V"
      },
      "source": [
        "## Datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ThXAIyUtOSQa"
      },
      "source": [
        "### DaNetQA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v1WEnl_hLCpj"
      },
      "source": [
        "random = Random_submission('DaNetQA')\r\n",
        "random.get_scores_random()\r\n",
        "majority.get_scores_majority()\r\n",
        "random_w.get_scores_random_weighted()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J4T64U3WOY8v"
      },
      "source": [
        "### RCB"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "03jeA6qjOhvP"
      },
      "source": [
        "random_RCB = Random_submission('RCB')\r\n",
        "random_RCB.get_scores_random()\r\n",
        "majority_RCB.get_scores_majority()\r\n",
        "random_w_RCB.get_scores_random_weighted()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TinpaoG7Omcc"
      },
      "source": [
        "### PARus"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rkIhBzEOOpwy"
      },
      "source": [
        "random_PARus = Random_submission('PARus')\r\n",
        "random_PARus.get_scores_random()\r\n",
        "majority_PARus.get_scores_majority()\r\n",
        "random_w_PARus.get_scores_random_weighted()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "osXHWsymOwyw"
      },
      "source": [
        "### TERRa"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_wAPpm7bO2L2"
      },
      "source": [
        "random_TERRa = Random_submission('TERRa')\r\n",
        "random_TERRa.get_scores_random()\r\n",
        "majority_TERRa.get_scores_majority()\r\n",
        "random_w_TERRa.get_scores_random_weighted()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1VjhRf8cPGAC"
      },
      "source": [
        "### RUSSE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h7ztK2mLPKS6"
      },
      "source": [
        "random_RUSSE = Random_submission('RUSSE')\r\n",
        "random_RUSSE.get_scores_random()\r\n",
        "majority_RUSSE.get_scores_majority()\r\n",
        "random_w_RUSSE.get_scores_random_weighted()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wNBcUdxmlt1S"
      },
      "source": [
        "### RWSD"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NNYkV3sFltHo"
      },
      "source": [
        "random_RWSD = Random_submission('RWSD')\r\n",
        "random_RWSD.get_scores_random()\r\n",
        "majority_RWSD.get_scores_majority()\r\n",
        "random_w_RWSD.get_scores_random_weighted()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SRp4kZ2_r-5Q"
      },
      "source": [
        "### LidiRus"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s2COVBIBsCUS"
      },
      "source": [
        "random_LiDiRus = Random_submission('LiDiRus', path = '/content/combined/TERRa/train.jsonl', path_valid='/content/combined/TERRa/val.jsonl',\r\n",
        "                                   path_test = '/content/combined/LiDiRus/LiDiRus.jsonl')\r\n",
        "random_LiDiRus.get_scores_random()\r\n",
        "majority_LiDiRus.get_scores_majority()\r\n",
        "random_w_LiDiRus.get_scores_random_weighted()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n9ABK_4MzNEc"
      },
      "source": [
        "# Optimised Tfidf"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2iVCypzz3Gu8"
      },
      "source": [
        "output_dir_tfidf = Path(\"tfidf_submission\")\r\n",
        "!mkdir $output_dir_tfidf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l2nmmekl1Pmn"
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression, SGDClassifier\r\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\r\n",
        "from sklearn.preprocessing import FunctionTransformer\r\n",
        "from sklearn.pipeline import Pipeline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c1BH--9D0JS1"
      },
      "source": [
        "def unite(path1, path2):\r\n",
        "  df = JSONL_handler(path1).to_pandas()\r\n",
        "  df1 = JSONL_handler(path2).to_pandas()\r\n",
        "  return pd.concat([df, df1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qEgnk8p61wiF"
      },
      "source": [
        "import json\r\n",
        "\r\n",
        "class Tfidf_Submisssion():\r\n",
        "  \r\n",
        "  def __init__(self, test, predictions, filename):\r\n",
        "    self.test = test\r\n",
        "    self.predictions = predictions\r\n",
        "    self.filename = filename + '.jsonl'\r\n",
        "\r\n",
        "  def test_output(self):\r\n",
        "    test_pred = [{\"idx\": idx, \"label\": str(label).lower()} for idx, label in zip(self.test.idx, self.predictions)]\r\n",
        "    self.save_output(test_pred, output_dir_tfidf / self.filename)\r\n",
        "\r\n",
        "  def save_output(self, data, path):\r\n",
        "    with open(path, mode=\"w\") as file:\r\n",
        "        for line in sorted(data, key=lambda x: int(x.get(\"idx\"))):\r\n",
        "            line[\"idx\"] = int(line[\"idx\"])\r\n",
        "            file.write(f\"{json.dumps(line, ensure_ascii=False)}\\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_oEkw7s77sXK"
      },
      "source": [
        "## Datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CNQmcII0z1Ej"
      },
      "source": [
        "### RCB"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QVq5FEnUz4s8"
      },
      "source": [
        "RCB_train = unite('/content/combined/RCB/train.jsonl', '/content/combined/RCB/val.jsonl')\r\n",
        "RCB_test = JSONL_handler('/content/combined/RCB/test.jsonl').to_pandas()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "raUj6PU0zSn_"
      },
      "source": [
        "steps_RCB = [('tfidf', TfidfVectorizer(analyzer='word', max_features=10000)),\r\n",
        "          ('func', FunctionTransformer(lambda x: x.todense(), accept_sparse=True)),\r\n",
        "         ('sgd', SGDClassifier(loss=\"log\", n_jobs=-1, alpha=0.00001, class_weight='balanced', random_state=42))]\r\n",
        "\r\n",
        "pipeline_RCB = Pipeline(steps_RCB)\r\n",
        "\r\n",
        "pipeline_RCB.fit(RCB_train.hypothesis, RCB_train.label)\r\n",
        "y_pred_RCB = pipeline_RCB.predict(RCB_test.hypothesis)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tsEEsUjf2Wtt"
      },
      "source": [
        "tfidf_rcb = Tfidf_Submisssion(RCB_test, y_pred_RCB, 'RCB')\r\n",
        "tfidf_rcb.test_output()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M12zdZwykq5G"
      },
      "source": [
        "### MuSeRC"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "txAw5OwoktzU"
      },
      "source": [
        "%%capture\r\n",
        "# RSG baseline class for MuSeRC\r\n",
        "!pip3 install jsonlines\r\n",
        "!wget -q --show-progress \"https://github.com/RussianNLP/RussianSuperGLUE/raw/master/tfidf_baseline/MuSeRC.py\" -O MuSeRC.py\r\n",
        "import MuSeRC"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QFTjlG_K9s4o"
      },
      "source": [
        "train_path = \"combined/MuSeRC/train.jsonl\"\r\n",
        "val_path = \"combined/MuSeRC/val.jsonl\"\r\n",
        "test_path = \"combined/MuSeRC/test.jsonl\"\r\n",
        "\r\n",
        "muserc = JSONL_handler(train_path).to_pandas()\r\n",
        "\r\n",
        "def extract_passages(row):\r\n",
        "    return row.get('text')\r\n",
        "\r\n",
        "muserc['text'] = muserc['passage'].apply(extract_passages)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ceaQ4hiIktr6"
      },
      "source": [
        "%%capture\r\n",
        "# These parameters show the highest result during tryouts\r\n",
        "vect = TfidfVectorizer(ngram_range=(1, 3), analyzer='char_wb', max_df = 0.8, max_features=5000)\r\n",
        "# Trained on most passages only to use consine_similarity with question+asnwer pairs\r\n",
        "vect.fit(muserc.text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "onCTTygssNn1",
        "outputId": "70f2221a-b95c-4d84-f9f7-ae591a95138b"
      },
      "source": [
        "_, MuSeRC_scores = MuSeRC.eval_MuSeRC(train_path, val_path, test_path, vect)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((0.3648602002071108, 0.6775232641374375),\n",
              " (0.3950850661625709, 0.6952705997074599))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YtOxlITZC5tr"
      },
      "source": [
        "scores_MuSeRC = MuSeRC_scores[\"test_pred\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p-FmUE9HslG8"
      },
      "source": [
        "tfidf_muserc = Tfidf_Submisssion(test_path, scores_MuSeRC, 'MuSeRC')\r\n",
        "tfidf_muserc.save_output(scores_MuSeRC, output_dir_tfidf / 'MuSeRC.jsonl')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tt16TWzM5bR3"
      },
      "source": [
        "### TERRa"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pdr7HuLi56CD"
      },
      "source": [
        "TERRa_train = unite('/content/combined/TERRa/train.jsonl', '/content/combined/TERRa/val.jsonl')\r\n",
        "TERRa_test = JSONL_handler('/content/combined/TERRa/test.jsonl').to_pandas()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yi87kVdr5dxY"
      },
      "source": [
        "steps_TERRa = [('countvect', CountVectorizer(min_df=15, max_df=0.4, lowercase=True, analyzer ='char_wb', decode_error = 'ignore', ngram_range = (2, 4))),\r\n",
        "         ('sgd', SGDClassifier(alpha = 1e-08, loss=\"log\", n_jobs=-1, class_weight='balanced', random_state=42))]\r\n",
        "\r\n",
        "pipeline_TERRa = Pipeline(steps_TERRa)\r\n",
        "\r\n",
        "pipeline_TERRa.fit(TERRa_train.hypothesis, TERRa_train.label)\r\n",
        "y_pred_TERRa = pipeline_TERRa.predict(TERRa_test.hypothesis)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hgfqX79x5_zE"
      },
      "source": [
        "tfidf_terra = Tfidf_Submisssion(TERRa_test, y_pred_TERRa, 'TERRa')\r\n",
        "tfidf_terra.test_output()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LlsZGLAu6U1C"
      },
      "source": [
        "### DaNetQA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kjj-SXgl6XpT"
      },
      "source": [
        "DaNetQA_train = unite('/content/combined/DaNetQA/train.jsonl', '/content/combined/DaNetQA/val.jsonl')\r\n",
        "DaNetQA_test = JSONL_handler('/content/combined/DaNetQA/test.jsonl').to_pandas()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TJ90YD9a6aBY"
      },
      "source": [
        "steps_DaNetQA = [('vectorizer', TfidfVectorizer()),\r\n",
        "              ('sgd', SGDClassifier(loss=\"log\", n_jobs=-1, alpha=0.15, class_weight='balanced', random_state=42))]\r\n",
        "\r\n",
        "pipeline_DaNetQA = Pipeline(steps_DaNetQA)\r\n",
        "\r\n",
        "pipeline_DaNetQA.fit(DaNetQA_train.question, DaNetQA_train.label)\r\n",
        "y_pred_DaNetQA = pipeline_DaNetQA.predict(DaNetQA_test.question)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M3mG5sHj7TdO"
      },
      "source": [
        "tfidf_danetqa = Tfidf_Submisssion(DaNetQA_test, y_pred_DaNetQA, 'DaNetQa')\r\n",
        "tfidf_danetqa.test_output()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vp4MWtzPOIUq"
      },
      "source": [
        "### RUSSE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v4Z6EI5WOC_j"
      },
      "source": [
        "RUSSE_train = unite('/content/combined/RUSSE/train.jsonl', '/content/combined/RUSSE/val.jsonl')\r\n",
        "RUSSE_test = JSONL_handler('/content/combined/RUSSE/test.jsonl').to_pandas()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bgxB7dsrOSfc"
      },
      "source": [
        "def build_feature_RUSSE(row):\r\n",
        "    sentence1 = row[\"sentence1\"].strip()\r\n",
        "    sentence2 = row[\"sentence2\"].strip()\r\n",
        "    word = row[\"word\"].strip()\r\n",
        "    res = f\"{sentence1} {sentence2} {word}\"\r\n",
        "    return res\r\n",
        "\r\n",
        "train_concat = []\r\n",
        "for i, row in RUSSE_train.iterrows():\r\n",
        "    train_concat.append(build_feature_RUSSE(row))\r\n",
        "RUSSE_train['concatenated'] = train_concat\r\n",
        "\r\n",
        "valid_concat = []\r\n",
        "for i, row in RUSSE_test.iterrows():\r\n",
        "    valid_concat.append(build_feature_RUSSE(row))\r\n",
        "RUSSE_test['concatenated'] =  valid_concat"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1BGlbWXjOg9Y"
      },
      "source": [
        "steps_RUSSE = [('tfidf', TfidfVectorizer(analyzer = 'word', max_df = 0.6, min_df= 0.001, ngram_range =  (1,2))),\r\n",
        "         ('logreg', LogisticRegression(C = 1.01, class_weight='balanced'))]\r\n",
        "\r\n",
        "pipeline_RUSSE = Pipeline(steps_RUSSE)\r\n",
        "\r\n",
        "pipeline_RUSSE.fit(RUSSE_train.concatenated, RUSSE_train.label)\r\n",
        "y_pred_RUSSE = pipeline_RUSSE.predict(RUSSE_test.concatenated)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tn4d1QvzPJBQ"
      },
      "source": [
        "tfidf_russe = Tfidf_Submisssion(RUSSE_test, y_pred_RUSSE, 'RUSSE')\r\n",
        "tfidf_russe.test_output()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HmTCOY9tSLHd"
      },
      "source": [
        "### PARus"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v7QvfQzJSRYD"
      },
      "source": [
        "PARus_train = unite('/content/combined/PARus/train.jsonl', '/content/combined/PARus/val.jsonl')\r\n",
        "PARus_test = JSONL_handler('/content/combined/PARus/test.jsonl').to_pandas()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q3DMGV2OSKiM"
      },
      "source": [
        "def build_feature_PARus(row):\r\n",
        "    premise = str(row[\"premise\"]).strip()\r\n",
        "    choice1 = row[\"choice1\"]\r\n",
        "    choice2 = row[\"choice2\"]\r\n",
        "    label = row.get(\"label\")\r\n",
        "    question = \"Что было ПРИЧИНОЙ этого?\" if row[\"question\"] == \"cause\" else \"Что случилось в РЕЗУЛЬТАТЕ?\"\r\n",
        "    res = f\"{premise} {question} {choice1} {choice2}\"\r\n",
        "    return res\r\n",
        "\r\n",
        "\r\n",
        "train_concat = []\r\n",
        "for i, row in PARus_train.iterrows():\r\n",
        "    train_concat.append(build_feature_PARus(row))\r\n",
        "PARus_train['concatenated'] = train_concat\r\n",
        "\r\n",
        "valid_concat = []\r\n",
        "for i, row in PARus_test.iterrows():\r\n",
        "    valid_concat.append(build_feature_PARus(row))\r\n",
        "PARus_test['concatenated'] =  valid_concat"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X_5yCz-aSdPw"
      },
      "source": [
        "steps_PARus = [('tfidf', TfidfVectorizer(analyzer= 'word', max_df= 0.6, min_df= 0.04, ngram_range= (1, 2))),\r\n",
        "         ('logreg', LogisticRegression(C = 1e-10, class_weight='balanced'))]\r\n",
        "\r\n",
        "pipeline_PARus = Pipeline(steps_PARus)\r\n",
        "\r\n",
        "pipeline_PARus.fit(PARus_train.concatenated, PARus_train.label)\r\n",
        "y_pred_PARus = pipeline_PARus.predict(PARus_test.concatenated)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EeXsS7HKS94r"
      },
      "source": [
        "tfidf_parus = Tfidf_Submisssion(PARus_test, y_pred_PARus, 'PARus')\r\n",
        "tfidf_parus.test_output()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1nn08m3ATJMW"
      },
      "source": [
        "### LiDiRus"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cD2-ovnrTL60"
      },
      "source": [
        "LiDiRus_train = unite('/content/combined/TERRa/train.jsonl', '/content/combined/TERRa/val.jsonl').assign(merged=lambda x: x.premise + \"\\n\" + x.hypothesis)\r\n",
        "LiDiRus_test = JSONL_handler('/content/combined/LiDiRus/LiDiRus.jsonl').to_pandas().assign(merged=lambda x: x.sentence1 + \"\\n\" + x.sentence2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5AhpFcZUVMMF"
      },
      "source": [
        "steps_LiDiRus = [('tfidf', TfidfVectorizer(analyzer= 'char_wb', max_df= 0.6, min_df=0.091, ngram_range = (1, 1))),\r\n",
        "         ('logreg', LogisticRegression(C = 1.01, class_weight='balanced'))]\r\n",
        "\r\n",
        "pipeline_LiDiRus = Pipeline(steps_LiDiRus)\r\n",
        "\r\n",
        "pipeline_LiDiRus.fit(LiDiRus_train.merged, LiDiRus_train.label)\r\n",
        "y_pred_LiDiRus = pipeline_LiDiRus.predict(LiDiRus_test.merged)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oBGa0LaKWHar"
      },
      "source": [
        "tfidf_lidirus = Tfidf_Submisssion(LiDiRus_test, y_pred_LiDiRus, 'LiDiRus')\r\n",
        "tfidf_lidirus.test_output()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s4n1h3Kje-1O"
      },
      "source": [
        "# Heuristics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eudc_bvWfFOg"
      },
      "source": [
        "output_dir_heuristics_random = Path(\"heuristics_random_submission\")\r\n",
        "!mkdir $output_dir_heuristics_random"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ER5L92uRnmqJ"
      },
      "source": [
        "output_dir_heuristics_majority = Path(\"heuristics_majority_submission\")\r\n",
        "!mkdir $output_dir_heuristics_majority"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T-uURRZ1nrb5"
      },
      "source": [
        "output_dir_heuristics_rw = Path(\"heuristics_rw_submission\")\r\n",
        "!mkdir $output_dir_heuristics_rw"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vkrdG2dCfIOA"
      },
      "source": [
        "import json\r\n",
        "\r\n",
        "class Heuristic_submission():\r\n",
        "  def __init__(self, dataset, solver, path = None, path_valid = None, path_test = None):\r\n",
        "    self.dataset = dataset\r\n",
        "    self.path = '/content/combined/' + dataset + '/train.jsonl' if path is None else path\r\n",
        "    self.path_valid = '/content/combined/' + dataset + '/val.jsonl' if path_valid is None else path_valid\r\n",
        "    self.path_test = '/content/combined/' + dataset + '/test.jsonl' if path_test is None else path_test\r\n",
        "    self.solver = solver(path=self.path, path_valid= self.path_valid, path_test = self.path_test)\r\n",
        "\r\n",
        "  def test_output(self):\r\n",
        "    test = JSONL_handler(self.path_test).to_pandas()\r\n",
        "    test_pred = [{\"idx\": idx, \"label\": str(label).lower()} for idx, label in zip(test.idx, self.scores)]\r\n",
        "    return test_pred\r\n",
        "\r\n",
        "  def get_scores_random(self):\r\n",
        "    self.scores = self.solver.heuristics_all(final_decision=self.solver.random_choice)\r\n",
        "    filename = self.dataset + \".jsonl\"\r\n",
        "    self.save_output(self.test_output(), output_dir_heuristics_random / filename)\r\n",
        "  \r\n",
        "  def get_scores_majority(self):\r\n",
        "    self.scores = self.solver.heuristics_all(final_decision=self.solver.majority_class)\r\n",
        "    filename = self.dataset + \".jsonl\"\r\n",
        "    self.save_output(self.test_output(), output_dir_heuristics_majority / filename)\r\n",
        "\r\n",
        "  def get_scores_random_weighted(self):\r\n",
        "    self.scores = self.solver.heuristics_all(final_decision=self.solver.random_balanced_choice)\r\n",
        "    filename = self.dataset + \".jsonl\"\r\n",
        "    self.save_output(self.test_output(), output_dir_heuristics_rw / filename)\r\n",
        "  \r\n",
        "  def save_output(self, data, path):\r\n",
        "    with open(path, mode=\"w\") as file:\r\n",
        "        for line in sorted(data, key=lambda x: int(x.get(\"idx\"))):\r\n",
        "            line[\"idx\"] = int(line[\"idx\"])\r\n",
        "            file.write(f\"{json.dumps(line, ensure_ascii=False)}\\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JGYEsphefzxH"
      },
      "source": [
        "## Datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Ck9wn9ViCGQ"
      },
      "source": [
        "%%capture\r\n",
        "!pip install pymorphy2\r\n",
        "!pip install razdel\r\n",
        "!pip install natasha"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YPL8k0EWwGEK"
      },
      "source": [
        "%%capture\r\n",
        "!wget \"https://github.com/RussianNLP/RussianSuperGLUE/raw/master/tfidf_baseline/LiDiRus.py\" -O LiDiRus.py\r\n",
        "!wget \"https://github.com/tatiana-iazykova/2020_HACK_RUSSIANSUPERGLUE/raw/main/utils.py\" -O utils.py\r\n",
        "!wget \"https://github.com/RussianNLP/RussianSuperGLUE/raw/master/tfidf_baseline/LiDiRus.py\" -O LiDiRus.py"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oHI10Lurf5L2"
      },
      "source": [
        "import pymorphy2\r\n",
        "import re\r\n",
        "from pymorphy2 import MorphAnalyzer\r\n",
        "import nltk\r\n",
        "from functools import lru_cache\r\n",
        "from base import BaseSolverSubmit\r\n",
        "from scipy import stats\r\n",
        "\r\n",
        "m = MorphAnalyzer()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-6Ag7Ivq8gcw"
      },
      "source": [
        "import numpy as np\r\n",
        "from natasha import (\r\n",
        "    Segmenter,\r\n",
        "    MorphVocab,  \r\n",
        "    NewsEmbedding,\r\n",
        "    NewsMorphTagger,\r\n",
        "    Doc\r\n",
        ")\r\n",
        "\r\n",
        "segmenter = Segmenter()\r\n",
        "morph_vocab = MorphVocab()\r\n",
        "emb = NewsEmbedding()\r\n",
        "morph_tagger = NewsMorphTagger(emb)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6whDZkEsf2Bb"
      },
      "source": [
        "### TERRa"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iZ-vKqCMf9Kx"
      },
      "source": [
        "class TERRaSolver(BaseSolverSubmit):\r\n",
        "   \r\n",
        "    def __init__(self, path: str, path_valid=None, path_test=None):\r\n",
        "        super(TERRaSolver, self).__init__(path, path_test, path_valid)\r\n",
        "   \r\n",
        "    def preprocess(self, columns):\r\n",
        "      for column in columns:\r\n",
        "        self.train[f\"{column}_lemmas\"] = self.train[column].apply(self.clean_text)\r\n",
        "        self.valid[f\"{column}_lemmas\"] = self.valid[column].apply(self.clean_text)\r\n",
        " \r\n",
        "    def words_only(self, text):\r\n",
        "      rg = re.compile(\"[А-Яа-яA-z]+\")\r\n",
        "      try:\r\n",
        "        return rg.findall(text.lower())\r\n",
        "      except:\r\n",
        "        return []\r\n",
        "\r\n",
        "    @lru_cache(maxsize=128)\r\n",
        "    def lemmatize_word(self, token, pymorphy=m):\r\n",
        "      return pymorphy.parse(token)[0].normal_form\r\n",
        "\r\n",
        "    def lemmatize_text(self, text):\r\n",
        "      return [self.lemmatize_word(w) for w in text]\r\n",
        "\r\n",
        "    def clean_text(self, text):\r\n",
        "      tokens = self.words_only(text)\r\n",
        "      lemmas = self.lemmatize_text(tokens)  \r\n",
        "      return lemmas\r\n",
        "    \r\n",
        "    def heuristics_all(self, final_decision=None):\r\n",
        "        y_pred = []\r\n",
        "        self.preprocess(columns=['premise', \"hypothesis\"])\r\n",
        "\r\n",
        "        for i, row in self.valid.iterrows():\r\n",
        "          \r\n",
        "          hyp = row.hypothesis.lower()\r\n",
        "          hyp_lem = set(row['hypothesis_lemmas'])\r\n",
        "          prem_lem = set(row['premise_lemmas'])\r\n",
        "          indic_non_ent = set(['только', 'мужчина'])\r\n",
        "\r\n",
        "          if hyp in row['premise'].lower():\r\n",
        "             y_pred.append('entailment')\r\n",
        "          elif len(prem_lem & hyp_lem)/len(hyp_lem) <= 1/3 or len(row['premise'].split()) < 29 or len(indic_non_ent & hyp_lem) > 0:\r\n",
        "            y_pred.append('not_entailment')\r\n",
        "          elif len(prem_lem & hyp_lem)/len(hyp_lem) == 0.75 or len(prem_lem & hyp_lem)/len(hyp_lem) == 1 or len(prem_lem & hyp_lem)/len(hyp_lem) == 2/3:\r\n",
        "            y_pred.append('entailment')\r\n",
        "          elif len(row['premise'].split()) > 32:\r\n",
        "            y_pred.append('entailment')\r\n",
        "          else:\r\n",
        "            y_pred.append(final_desicion(test_size=1)[0])\r\n",
        "        \r\n",
        "        return y_pred"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JNxAWwYxxlfZ"
      },
      "source": [
        "terra_heuristics = Heuristic_submission('TERRa', TERRaSolver)\r\n",
        "terra_heuristics.get_scores_random()\r\n",
        "terra_heuristics.get_scores_majority()\r\n",
        "terra_heuristics.get_scores_random_weighted()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cJSjaVPI62kX"
      },
      "source": [
        "### DaNetQA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dPU9TbPG66nu"
      },
      "source": [
        "class DaNetQASolver(BaseSolverSubmit):\r\n",
        "    \r\n",
        "    def __init__(self, path: str, path_valid=None, path_test=None):\r\n",
        "        super(DaNetQASolver, self).__init__(path, path_test, path_valid)\r\n",
        "   \r\n",
        "    def heuristics_all(self, final_decision=None):\r\n",
        "        y_pred = []\r\n",
        "\r\n",
        "        for i, row in self.valid.iterrows():\r\n",
        "\r\n",
        "            question = row.question.lower()\r\n",
        "            question_w_count = len(question.split())\r\n",
        "            passage_w_count = len(row.passage.split())\r\n",
        "\r\n",
        "            if re.search(\"был|(^есть)\", question):\r\n",
        "              y_pred.append(True)\r\n",
        "            elif re.search(\"^входит|едят|правда ли\", question):\r\n",
        "              y_pred.append(False)\r\n",
        "            elif question_w_count > 5:\r\n",
        "              y_pred.append(False)\r\n",
        "            elif passage_w_count >= 90:\r\n",
        "              y_pred.append(False)\r\n",
        "            else:\r\n",
        "              y_pred.append(final_desicion(test_size=1)[0])\r\n",
        "      \r\n",
        "        return y_pred"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qJrSBwrU7R6U"
      },
      "source": [
        "danetqa_heuristics = Heuristic_submission('DaNetQA', DaNetQASolver)\r\n",
        "danetqa_heuristics.get_scores_random()\r\n",
        "danetqa_heuristics.get_scores_majority()\r\n",
        "danetqa_heuristics.get_scores_random_weighted()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZFkkBWKb7s0_"
      },
      "source": [
        "### RCB"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hB_hgyyy7v9o"
      },
      "source": [
        "class RCBSolver(BaseSolverSubmit):\r\n",
        "\r\n",
        "    def __init__(self, path: str, path_valid=None, path_test=None):\r\n",
        "        super(RCBSolver, self).__init__(path, path_test, path_valid)\r\n",
        "   \r\n",
        "    def preprocess(self, columns):\r\n",
        "      for column in columns:\r\n",
        "        self.train[f\"{column}_lemmas\"] = self.train[column].apply(self.clean_text)\r\n",
        "        self.valid[f\"{column}_lemmas\"] = self.valid[column].apply(self.clean_text)\r\n",
        " \r\n",
        "    def words_only(self, text):\r\n",
        "      rg = re.compile(\"[А-Яа-яA-z]+\")\r\n",
        "      try:\r\n",
        "        return rg.findall(text.lower())\r\n",
        "      except:\r\n",
        "        return []\r\n",
        "\r\n",
        "    @lru_cache(maxsize=128)\r\n",
        "    def lemmatize_word(self, token, pymorphy=m):\r\n",
        "      return pymorphy.parse(token)[0].normal_form\r\n",
        "\r\n",
        "    def lemmatize_text(self, text):\r\n",
        "      return [self.lemmatize_word(w) for w in text]\r\n",
        "\r\n",
        "    def clean_text(self, text):\r\n",
        "      tokens = self.words_only(text)\r\n",
        "      lemmas = self.lemmatize_text(tokens)  \r\n",
        "      return lemmas\r\n",
        "    \r\n",
        "    def heuristics_all(self, final_decision=None):\r\n",
        "        y_pred = []\r\n",
        "        self.preprocess(columns=['premise', \"hypothesis\"])\r\n",
        "\r\n",
        "        for i, row in self.valid.iterrows():\r\n",
        "          \r\n",
        "          hyp = row.hypothesis.lower()\r\n",
        "          hyp_lem = set(row['hypothesis_lemmas'])\r\n",
        "          prem_lem = set(row['premise_lemmas'])\r\n",
        "          indic_neutral = set(['подозревать', 'cчитать', 'говорить', 'думать', 'надеяться', 'понять', 'уверять'])\r\n",
        "          indic_ent = set(['признать'])\r\n",
        "\r\n",
        "          if hyp in row['premise'].lower() or len(indic_ent & prem_lem) > 0 :\r\n",
        "             y_pred.append('entailment')\r\n",
        "          elif len(prem_lem & hyp_lem)/len(hyp_lem) == 0.75:\r\n",
        "            y_pred.append('entailment')\r\n",
        "          elif len(indic_neutral & prem_lem) > 0:\r\n",
        "            y_pred.append('neutral')\r\n",
        "          elif len(row.hypothesis.split()) < 5:\r\n",
        "            y_pred.append('contradiction')\r\n",
        "          elif len(row.hypothesis.split()) >= 5 and len(row.hypothesis.split()) <= 7:\r\n",
        "            y_pred.append('neutral')\r\n",
        "          elif len(row['premise'].split()) > 30:\r\n",
        "            y_pred.append('entailment')\r\n",
        "          else:\r\n",
        "            y_pred.append(final_desicion(test_size=1)[0])\r\n",
        "        \r\n",
        "        return y_pred"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VHR8UdQR7wUj"
      },
      "source": [
        "rcb_heuristics = Heuristic_submission('RCB', RCBSolver)\r\n",
        "rcb_heuristics.get_scores_random()\r\n",
        "rcb_heuristics.get_scores_majority()\r\n",
        "rcb_heuristics.get_scores_random_weighted()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B8UVEo188Sz6"
      },
      "source": [
        "### PARus"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0PygoCcV8ZUc"
      },
      "source": [
        "class ParusSolver(BaseSolverSubmit):\r\n",
        "   \r\n",
        "    def __init__(self, path: str, path_valid=None, path_test=None):\r\n",
        "        super(ParusSolver, self).__init__(path, path_test, path_valid)\r\n",
        "   \r\n",
        "    def preprocess(self, columns):\r\n",
        "\r\n",
        "        for column in columns:\r\n",
        "            self.train[f\"{column}_lemmas\"] = self.train[column].apply(self.lemmatize)\r\n",
        "            self.valid[f\"{column}_lemmas\"] = self.valid[column].apply(self.lemmatize)\r\n",
        "\r\n",
        "    def lemmatize(self, text):\r\n",
        "        \"\"\"\r\n",
        "        param text: str\r\n",
        "        return: List of lemmas (strings)\r\n",
        "        \"\"\"\r\n",
        "\r\n",
        "        doc = Doc(text)\r\n",
        "        doc.segment(segmenter)\r\n",
        "        doc.tag_morph(morph_tagger)\r\n",
        "\r\n",
        "        for token in doc.tokens:\r\n",
        "            token.lemmatize(morph_vocab)\r\n",
        "        lemmas = [token.lemma for token in doc.tokens]\r\n",
        "        return lemmas\r\n",
        "\r\n",
        "    \r\n",
        "    def heuristics_all(self, final_decision=None):\r\n",
        "        \"\"\"\r\n",
        "        This heruistic chooses the option that has more common lemmas with premise\r\n",
        "        If the amount of common words is equal for both choices, it uses {final_desicion}\r\n",
        "        function (one of BaseSolver functions) to predict\r\n",
        "        param: final_decision (function)\r\n",
        "        \"\"\"\r\n",
        "        y_pred = []\r\n",
        "        self.preprocess(columns=['premise', 'choice1', 'choice2'])\r\n",
        "\r\n",
        "        for i, row in self.valid.iterrows():\r\n",
        "            words1 = set(row.choice1_lemmas)\r\n",
        "            words2 = set(row.choice2_lemmas)\r\n",
        "            premise = set(row.premise_lemmas)\r\n",
        "            overlap1 = len(premise & words1)\r\n",
        "            overlap2 = len(premise & words2)\r\n",
        "            if overlap1 > overlap2:\r\n",
        "                y_pred.append(0)\r\n",
        "            elif overlap2 > overlap1:\r\n",
        "                y_pred.append(1)\r\n",
        "            else:\r\n",
        "                y_pred.append(final_desicion(test_size=1)[0])\r\n",
        "        return y_pred"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BY95BGQm80DP"
      },
      "source": [
        "parus_heuristics = Heuristic_submission('PARus', ParusSolver)\r\n",
        "parus_heuristics.get_scores_random()\r\n",
        "parus_heuristics.get_scores_majority()\r\n",
        "parus_heuristics.get_scores_random_weighted()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PZWlrntBdNz3"
      },
      "source": [
        "### RUSSE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GbHYMb8DdNY_"
      },
      "source": [
        "class RusseSolver(BaseSolverSubmit):\r\n",
        "\r\n",
        "    def __init__(self, path: str, path_valid=None, path_test=None):\r\n",
        "        super(RusseSolver, self).__init__(path, path_test, path_valid)\r\n",
        "\r\n",
        "    def heuristics_all(self, final_decision=None):\r\n",
        "        y_pred = []\r\n",
        "\r\n",
        "        for i, row in self.valid.iterrows():\r\n",
        "            tokens1 = set(row.sentence1.split())\r\n",
        "            tokens2 = set(row.sentence2.split())\r\n",
        "\r\n",
        "            if len(tokens1 & tokens2) / len(tokens1 | tokens2) > 0.10:\r\n",
        "                y_pred.append(True)\r\n",
        "            else:\r\n",
        "                options = np.array([final_decision(test_size=1)[0] for i in range(0,3)])\r\n",
        "                y_pred.append(stats.mode(options)[0][0])\r\n",
        "        return y_pred"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7_OJeqwCqu3s"
      },
      "source": [
        "russe_heuristics = Heuristic_submission('RUSSE', RusseSolver)\r\n",
        "russe_heuristics.get_scores_random()\r\n",
        "russe_heuristics.get_scores_majority()\r\n",
        "russe_heuristics.get_scores_random_weighted()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mPyV7M0Ckv0C"
      },
      "source": [
        "### LiDiRus"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W9D35jmG4R-g"
      },
      "source": [
        "%%capture\r\n",
        "# to lemmantize, install these two dependencies\r\n",
        "!pip3 install pyMorphy2[fast]\r\n",
        "!pip3 install razdel"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VaP0M2qdxDbC"
      },
      "source": [
        "import re\r\n",
        "from scipy import stats\r\n",
        "from string import punctuation\r\n",
        "from pymorphy2 import MorphAnalyzer\r\n",
        "from razdel import tokenize as razdel_tokenize\r\n",
        "from base import BaseSolver\r\n",
        "from utils import RSG_MorphAnalyzer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JGfQ0NDGkw0k"
      },
      "source": [
        "class LiDiRusSolver(BaseSolver):\r\n",
        "\r\n",
        "    def __init__(self, path: str, path_valid=None):\r\n",
        "        self.e_words = {\"чтобы\", 'будет', \"от\", \"он\"} # -> entailment\r\n",
        "        self.ne_words = {'и', \"не\", \"никогда\", \"вовсе\", 'что', \"это\"} # -> not_eintailment\r\n",
        "        self.morph = RSG_MorphAnalyzer() # PyMorphy + cashing\r\n",
        "        super(LiDiRusSolver, self).__init__(path, path_valid)\r\n",
        "\r\n",
        "    def preprocess(self):\r\n",
        "        self.cashe = {} # create a dictionary for lemmas\r\n",
        "        \"\"\" preprocess sentences to apply heuristics\"\"\"\r\n",
        "        self.valid[\"sentence1_words\"] = self.valid['sentence1'].str.split()\r\n",
        "        self.valid[\"sentence2_words\"] = self.valid['sentence2'].str.split()\r\n",
        "        self.valid[\"sentence1_lemmas\"] = self.morph.lemmantize_sentences(self.valid.sentence1.to_list())\r\n",
        "        self.valid[\"sentence2_lemmas\"] = self.morph.lemmantize_sentences(self.valid.sentence2.to_list())\r\n",
        "\r\n",
        "    def get_heuristics(self, non_intersect, intersect, non_intersect_lemmas, heuristic) -> dict:\r\n",
        "        \"\"\" all heuristics at once or one of them \"\"\"\r\n",
        "\r\n",
        "        heuristics = {\r\n",
        "            \"not_entailment\": {\r\n",
        "                \"little overlap\": len(non_intersect) > 10,\r\n",
        "\r\n",
        "                # catches if there is an extra clause inside\r\n",
        "                \"extra clause\": len(re.findall(r\",\", \" \".join(non_intersect))) > 1,\r\n",
        "\r\n",
        "                \"keyword\": len(non_intersect) == 2,\r\n",
        "\r\n",
        "                # negated word, e.g: необычный, незапланированно\r\n",
        "                # \"negated words\": re.search(r'(?<=\\s)не\\w+', \" \".join(non_intersect)) != None ,\r\n",
        "\r\n",
        "                # has one of the words from the list\r\n",
        "                \"wordlist\": len(self.ne_words.intersection(non_intersect)) > 0},\r\n",
        "\r\n",
        "            \"entailment\": {\r\n",
        "                \"all lemmas overlap\": len(non_intersect_lemmas) == 0,\r\n",
        "\r\n",
        "                \"wordlist\": len(self.e_words.intersection(intersect)) > 0}}\r\n",
        "\r\n",
        "        if heuristic != None:\r\n",
        "            # return a single heuristic only\r\n",
        "            key = list(heuristic.keys())[0]\r\n",
        "            value = heuristic[key]\r\n",
        "\r\n",
        "            return ({\r\n",
        "                key: { # key = \"entailment\" or \"not_entailment\"\r\n",
        "                      value: heuristics[key][value] # \"heuristic name\": Boolean\r\n",
        "                      }\r\n",
        "                    })\r\n",
        "        return (heuristics)\r\n",
        "\r\n",
        "    def heuristics_all(self, final_decision = None, heuristic = None):\r\n",
        "        \"\"\"\r\n",
        "            apply heuristics to a dataset\r\n",
        "            To check on a single heursitic, pass\r\n",
        "                        heuristic = {\"label\": \"heuristic name\"}\r\n",
        "            to this function\r\n",
        "        \"\"\"\r\n",
        "        y_pred = []\r\n",
        "\r\n",
        "\r\n",
        "        for i, row in self.valid.iterrows():\r\n",
        "\r\n",
        "            sentence1 = row['sentence1_words']\r\n",
        "            sentence2 = row['sentence2_words']\r\n",
        "\r\n",
        "            non_intersect = set(sentence1) ^ set(sentence2)\r\n",
        "            intersect = set(sentence1).intersection(sentence2)\r\n",
        "            lemmas_non_intersect = set(row.sentence1_lemmas) ^ set(row.sentence2_lemmas)\r\n",
        "\r\n",
        "            heuristics = self.get_heuristics(non_intersect,\r\n",
        "                                             intersect,\r\n",
        "                                             lemmas_non_intersect,\r\n",
        "                                             heuristic)\r\n",
        "\r\n",
        "\r\n",
        "            if ('entailment' in heuristics.keys() and\r\n",
        "                (True in list(heuristics['entailment'].values()))):\r\n",
        "                    y_pred.append('entailment')\r\n",
        "            elif ('not_entailment' in heuristics.keys() and\r\n",
        "                (True in list(heuristics['not_entailment'].values()))):\r\n",
        "                y_pred.append('not_entailment') # inserts an opposite label\r\n",
        "            else:\r\n",
        "                y_pred.append(final_decision(test_size=1)[0])\r\n",
        "\r\n",
        "        return y_pred"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CPXeyMrr0-Yz"
      },
      "source": [
        "solver = LiDiRusSolver(path='/content/combined/LiDiRus/LiDiRus.jsonl', path_valid='/content/combined/LiDiRus/LiDiRus.jsonl')\r\n",
        "solver.preprocess()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yh9IW4UT5PWp"
      },
      "source": [
        "test_LiDiRus = JSONL_handler('/content/combined/LiDiRus/LiDiRus.jsonl').to_pandas()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2-D-Yki3kxDE"
      },
      "source": [
        "LiDiRus_majority = [{\"idx\": idx, \"label\": str(label).lower()} for idx, label in zip(test_LiDiRus.idx, solver.heuristics_all(final_decision=solver.majority_class))]\r\n",
        "LiDiRus_random = [{\"idx\": idx, \"label\": str(label).lower()} for idx, label in zip(test_LiDiRus.idx, solver.heuristics_all(final_decision=solver.random_choice))]\r\n",
        "LiDiRus_random_b = [{\"idx\": idx, \"label\": str(label).lower()} for idx, label in zip(test_LiDiRus.idx, solver.heuristics_all(final_decision=solver.random_balanced_choice))]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HePJ3rcp5oas"
      },
      "source": [
        "import json\r\n",
        "def save_output(data, path):\r\n",
        "    with open(path, mode=\"w\") as file:\r\n",
        "        for line in sorted(data, key=lambda x: int(x.get(\"idx\"))):\r\n",
        "            line[\"idx\"] = int(line[\"idx\"])\r\n",
        "            file.write(f\"{json.dumps(line, ensure_ascii=False)}\\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_mQwrWLz5BQU"
      },
      "source": [
        "save_output(LiDiRus_majority, output_dir_heuristics_majority / \"LiDiRus.jsonl\")\r\n",
        "save_output(LiDiRus_random, output_dir_heuristics_random / \"LiDiRus.jsonl\")\r\n",
        "save_output(LiDiRus_random_b, output_dir_heuristics_rw / \"LiDiRus.jsonl\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MsXUcjEXuWWL"
      },
      "source": [
        "### MuSeRC"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VUpHwvTgJbC5"
      },
      "source": [
        "%%capture\r\n",
        "# to lemmantize, install these two dependancies\r\n",
        "!pip3 install pyMorphy2[fast]\r\n",
        "!pip3 install razdel\r\n",
        "!pip3 install jsonlines"
      ],
      "execution_count": 177,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FIFVmQJMJ6cM",
        "outputId": "671b0bc5-e52c-4fed-8aa8-8497360ddc93"
      },
      "source": [
        "!wget -q --show-progress \"https://github.com/tatiana-iazykova/2020_HACK_RUSSIANSUPERGLUE/raw/main/Solvers/MuSeRCSolver.py\" -O MuSeRCSolver.py"
      ],
      "execution_count": 178,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\rMuSeRCSolver.py       0%[                    ]       0  --.-KB/s               \rMuSeRCSolver.py     100%[===================>]   8.83K  --.-KB/s    in 0s      \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KJHkAs6wJ-8I"
      },
      "source": [
        "from MuSeRCSolver import MuSeRCSolver\r\n",
        "from MuSeRC import MuSeRC_metrics\r\n",
        "\r\n",
        "solver = MuSeRCSolver('/content/combined/MuSeRC/train.jsonl', path_valid='/content/combined/MuSeRC/val.jsonl') # pass a dataset to get stats\r\n",
        "solver.preprocess_data('/content/combined/MuSeRC/test.jsonl')\r\n",
        "solver.get_stats_MuSeRC() "
      ],
      "execution_count": 188,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MUp6TEkQQDZ3"
      },
      "source": [
        "import json\r\n",
        "\r\n",
        "def save_output(data, path):\r\n",
        "    with open(path, mode=\"w\") as file:\r\n",
        "        for line in sorted(data, key=lambda x: int(x.get(\"idx\"))):\r\n",
        "            line[\"idx\"] = int(line[\"idx\"])\r\n",
        "            file.write(f\"{json.dumps(line, ensure_ascii=False)}\\n\")"
      ],
      "execution_count": 189,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B4GCdjPTQOD0",
        "outputId": "3de0120d-5845-4764-9f61-be9c1becfd55"
      },
      "source": [
        "scores_muserc, _, _ = solver.heuristics()\r\n",
        "scores_muserc_r, _, _ = solver.heuristics('RANDOM')\r\n",
        "scores_muserc_rb, _, _ = solver.heuristics('RB')"
      ],
      "execution_count": 190,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Heuristics appears for 5909 samples, 2001 of them correct\n",
            "Heuristics appears for 5909 samples, 2001 of them correct\n",
            "Heuristics appears for 5909 samples, 2001 of them correct\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 362
        },
        "id": "HXSt6RFVQuU6",
        "outputId": "654c4949-5a8f-402b-aa1b-95f106c6399f"
      },
      "source": [
        "save_output(scores_muserc, output_dir_heuristics_majority / \"MuSeRC.jsonl\")\r\n",
        "save_output(scores_muserc_r, output_dir_heuristics_random / \"MuSeRC.jsonl\")\r\n",
        "save_output(scores_muserc_rb, output_dir_heuristics_rw / \"MuSeRC.jsonl\")"
      ],
      "execution_count": 191,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-191-37b44cd28693>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0msave_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores_muserc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_dir_heuristics_majority\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m\"MuSeRC.jsonl\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0msave_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores_muserc_r\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_dir_heuristics_random\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m\"MuSeRC.jsonl\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0msave_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores_muserc_rb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_dir_heuristics_rw\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m\"MuSeRC.jsonl\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-189-d38f475eb1b5>\u001b[0m in \u001b[0;36msave_output\u001b[0;34m(data, path)\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"idx\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m             \u001b[0mline\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"idx\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"idx\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m             \u001b[0mfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{json.dumps(line, ensure_ascii=False)}\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/lib/python3.6/json/__init__.py\u001b[0m in \u001b[0;36mdumps\u001b[0;34m(obj, skipkeys, ensure_ascii, check_circular, allow_nan, cls, indent, separators, default, sort_keys, **kw)\u001b[0m\n\u001b[1;32m    236\u001b[0m         \u001b[0mcheck_circular\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcheck_circular\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_nan\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mallow_nan\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mindent\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m         \u001b[0mseparators\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mseparators\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdefault\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdefault\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msort_keys\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msort_keys\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 238\u001b[0;31m         **kw).encode(obj)\n\u001b[0m\u001b[1;32m    239\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/json/encoder.py\u001b[0m in \u001b[0;36mencode\u001b[0;34m(self, o)\u001b[0m\n\u001b[1;32m    197\u001b[0m         \u001b[0;31m# exceptions aren't as detailed.  The list call should be roughly\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m         \u001b[0;31m# equivalent to the PySequence_Fast that ''.join() would do.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m         \u001b[0mchunks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_one_shot\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m             \u001b[0mchunks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/json/encoder.py\u001b[0m in \u001b[0;36miterencode\u001b[0;34m(self, o, _one_shot)\u001b[0m\n\u001b[1;32m    255\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkey_separator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem_separator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort_keys\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m                 self.skipkeys, _one_shot)\n\u001b[0;32m--> 257\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_iterencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m def _make_iterencode(markers, _default, _encoder, _indent, _floatstr,\n",
            "\u001b[0;32m/usr/lib/python3.6/json/encoder.py\u001b[0m in \u001b[0;36mdefault\u001b[0;34m(self, o)\u001b[0m\n\u001b[1;32m    178\u001b[0m         \"\"\"\n\u001b[1;32m    179\u001b[0m         raise TypeError(\"Object of type '%s' is not JSON serializable\" %\n\u001b[0;32m--> 180\u001b[0;31m                         o.__class__.__name__)\n\u001b[0m\u001b[1;32m    181\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: Object of type 'int64' is not JSON serializable"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wq1eLrv8dwye"
      },
      "source": [
        "# Make submission file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S16wnAe7eBMq"
      },
      "source": [
        "!7z a \"random_submission.zip\" $output_dir\r\n",
        "!7z a \"majority_submission.zip\" $output_dir_majority\r\n",
        "!7z a \"random_weighted_submission.zip\" $output_dir_random_weighted"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xCnSTIme7v8K"
      },
      "source": [
        "!7z a \"random_weighted_submission.zip\" $output_dir_tfidf"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}