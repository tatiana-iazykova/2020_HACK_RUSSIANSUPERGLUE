{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "generate_json.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "JxRpQnEwRB3V",
        "n9ABK_4MzNEc",
        "_oEkw7s77sXK",
        "6whDZkEsf2Bb",
        "Wq1eLrv8dwye"
      ],
      "authorship_tag": "ABX9TyNWresGWE6O6H5+e89gC91y",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tatiana-iazykova/2020_HACK_RUSSIANSUPERGLUE/blob/main/generate_json.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7lPbpWjqF3t_",
        "outputId": "2aa9806c-cadc-4a62-befd-b253256ee1ec"
      },
      "source": [
        "!wget -q --show-progress \"https://raw.githubusercontent.com/tatiana-iazykova/2020_HACK_RUSSIANSUPERGLUE/main/base.py\" -O base.py"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\rbase.py               0%[                    ]       0  --.-KB/s               \rbase.py             100%[===================>]   3.44K  --.-KB/s    in 0s      \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1f5wl93tIuLt"
      },
      "source": [
        "%%capture\r\n",
        "!wget https://russiansuperglue.com/tasks/download\r\n",
        "!unzip download\r\n",
        "!rm download\r\n",
        "!rm -r /content/__MACOSX\r\n",
        "!rm -r sample_data/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q9vbghmVGEJE"
      },
      "source": [
        "from pathlib import Path\n",
        "data_dir = Path(\"combined/\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m9h5aNHOJu4p"
      },
      "source": [
        "import pandas as pd\r\n",
        "\r\n",
        "class JSONL_handler():\r\n",
        "    \"\"\" opens a jsonl file and turns it into a necessary data structure \"\"\"\r\n",
        "    \r\n",
        "    def __init__(self, path):\r\n",
        "        self.path = path # path to jsonl file\r\n",
        "\r\n",
        "    def to_pandas(self):\r\n",
        "        \"\"\" get jsonl file content as a pandas DataFrame\"\"\"\r\n",
        "        return pd.read_json(path_or_buf=self.path, lines=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IwtBjRQwOEhE"
      },
      "source": [
        "# Baseline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oeikZ_jjGN2g"
      },
      "source": [
        "output_dir = Path(\"random_submission\")\n",
        "!mkdir $output_dir"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MiJ5Xvq7N9g5"
      },
      "source": [
        "output_dir_majority = Path(\"majority_submission\")\r\n",
        "!mkdir $output_dir_majority"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hLUxyRPbOBQS"
      },
      "source": [
        "output_dir_random_weighted = Path(\"random_weighted_submission\")\r\n",
        "!mkdir $output_dir_random_weighted"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l7wP1Ix-LIjC"
      },
      "source": [
        "from base import BaseSolverSubmit\r\n",
        "import json\r\n",
        "\r\n",
        "class Random_submission():\r\n",
        "  def __init__(self, dataset, path = None, path_valid = None, path_test = None):\r\n",
        "    self.dataset = dataset\r\n",
        "    self.path = '/content/combined/' + dataset + '/train.jsonl' if path is None else path\r\n",
        "    self.path_valid = '/content/combined/' + dataset + '/val.jsonl' if path_valid is None else path_valid\r\n",
        "    self.path_test = '/content/combined/' + dataset + '/test.jsonl' if path_test is None else path_test\r\n",
        "\r\n",
        "  def test_output(self):\r\n",
        "    test = JSONL_handler(self.path_test).to_pandas()\r\n",
        "    test_pred = [{\"idx\": idx, \"label\": str(label).lower()} for idx, label in zip(test.idx, self.scores)]\r\n",
        "    return test_pred\r\n",
        "\r\n",
        "  def get_scores_random(self):\r\n",
        "    solver = BaseSolverSubmit(path = self.path, path_valid = self.path_valid, path_test = self.path_test)\r\n",
        "    self.scores = solver.random_choice(len(solver.valid))\r\n",
        "    filename = self.dataset + \".jsonl\"\r\n",
        "    self.save_output(self.test_output(), output_dir / filename)\r\n",
        "  \r\n",
        "  def get_scores_majority(self):\r\n",
        "    solver = BaseSolverSubmit(path = self.path, path_valid = self.path_valid, path_test = self.path_test)\r\n",
        "    self.scores = solver.majority_class(len(solver.valid))\r\n",
        "    filename = self.dataset + \".jsonl\"\r\n",
        "    self.save_output(self.test_output(), output_dir_majority / filename)\r\n",
        "\r\n",
        "  def get_scores_random_weighted(self):\r\n",
        "    solver = BaseSolverSubmit(path = self.path, path_valid = self.path_valid, path_test = self.path_test)\r\n",
        "    self.scores = solver.random_balanced_choice(len(solver.valid))\r\n",
        "    filename = self.dataset + \".jsonl\"\r\n",
        "    self.save_output(self.test_output(), output_dir_random_weighted / filename)\r\n",
        "  \r\n",
        "  def save_output(self, data, path):\r\n",
        "    with open(path, mode=\"w\") as file:\r\n",
        "        for line in sorted(data, key=lambda x: int(x.get(\"idx\"))):\r\n",
        "            line[\"idx\"] = int(line[\"idx\"])\r\n",
        "            file.write(f\"{json.dumps(line, ensure_ascii=False)}\\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JxRpQnEwRB3V"
      },
      "source": [
        "## Datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ThXAIyUtOSQa"
      },
      "source": [
        "### DaNetQA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v1WEnl_hLCpj"
      },
      "source": [
        "random = Random_submission('DaNetQA')\r\n",
        "random.get_scores_random()\r\n",
        "majority.get_scores_majority()\r\n",
        "random_w.get_scores_random_weighted()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J4T64U3WOY8v"
      },
      "source": [
        "### RCB"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "03jeA6qjOhvP"
      },
      "source": [
        "random_RCB = Random_submission('RCB')\r\n",
        "random_RCB.get_scores_random()\r\n",
        "majority_RCB.get_scores_majority()\r\n",
        "random_w_RCB.get_scores_random_weighted()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TinpaoG7Omcc"
      },
      "source": [
        "### PARus"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rkIhBzEOOpwy"
      },
      "source": [
        "random_PARus = Random_submission('PARus')\r\n",
        "random_PARus.get_scores_random()\r\n",
        "majority_PARus.get_scores_majority()\r\n",
        "random_w_PARus.get_scores_random_weighted()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "osXHWsymOwyw"
      },
      "source": [
        "### TERRa"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_wAPpm7bO2L2"
      },
      "source": [
        "random_TERRa = Random_submission('TERRa')\r\n",
        "random_TERRa.get_scores_random()\r\n",
        "majority_TERRa.get_scores_majority()\r\n",
        "random_w_TERRa.get_scores_random_weighted()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1VjhRf8cPGAC"
      },
      "source": [
        "### RUSSE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h7ztK2mLPKS6"
      },
      "source": [
        "random_RUSSE = Random_submission('RUSSE')\r\n",
        "random_RUSSE.get_scores_random()\r\n",
        "majority_RUSSE.get_scores_majority()\r\n",
        "random_w_RUSSE.get_scores_random_weighted()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wNBcUdxmlt1S"
      },
      "source": [
        "### RWSD"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NNYkV3sFltHo"
      },
      "source": [
        "random_RWSD = Random_submission('RWSD')\r\n",
        "random_RWSD.get_scores_random()\r\n",
        "majority_RWSD.get_scores_majority()\r\n",
        "random_w_RWSD.get_scores_random_weighted()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SRp4kZ2_r-5Q"
      },
      "source": [
        "### LidiRus"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s2COVBIBsCUS"
      },
      "source": [
        "random_LiDiRus = Random_submission('LiDiRus', path = '/content/combined/TERRa/train.jsonl', path_valid='/content/combined/TERRa/val.jsonl',\r\n",
        "                                   path_test = '/content/combined/LiDiRus/LiDiRus.jsonl')\r\n",
        "random_LiDiRus.get_scores_random()\r\n",
        "majority_LiDiRus.get_scores_majority()\r\n",
        "random_w_LiDiRus.get_scores_random_weighted()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n9ABK_4MzNEc"
      },
      "source": [
        "# Optimised Tfidf"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2iVCypzz3Gu8"
      },
      "source": [
        "output_dir_tfidf = Path(\"tfidf_submission\")\r\n",
        "!mkdir $output_dir_tfidf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l2nmmekl1Pmn"
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression, SGDClassifier\r\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\r\n",
        "from sklearn.preprocessing import FunctionTransformer\r\n",
        "from sklearn.pipeline import Pipeline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c1BH--9D0JS1"
      },
      "source": [
        "def unite(path1, path2):\r\n",
        "  df = JSONL_handler(path1).to_pandas()\r\n",
        "  df1 = JSONL_handler(path2).to_pandas()\r\n",
        "  return pd.concat([df, df1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qEgnk8p61wiF"
      },
      "source": [
        "import json\r\n",
        "\r\n",
        "class Tfidf_Submisssion():\r\n",
        "  \r\n",
        "  def __init__(self, test, predictions, filename):\r\n",
        "    self.test = test\r\n",
        "    self.predictions = predictions\r\n",
        "    self.filename = filename + '.jsonl'\r\n",
        "\r\n",
        "  def test_output(self):\r\n",
        "    test_pred = [{\"idx\": idx, \"label\": str(label).lower()} for idx, label in zip(self.test.idx, self.predictions)]\r\n",
        "    self.save_output(test_pred, output_dir_tfidf / self.filename)\r\n",
        "\r\n",
        "  def save_output(self, data, path):\r\n",
        "    with open(path, mode=\"w\") as file:\r\n",
        "        for line in sorted(data, key=lambda x: int(x.get(\"idx\"))):\r\n",
        "            line[\"idx\"] = int(line[\"idx\"])\r\n",
        "            file.write(f\"{json.dumps(line, ensure_ascii=False)}\\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_oEkw7s77sXK"
      },
      "source": [
        "## Datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CNQmcII0z1Ej"
      },
      "source": [
        "### RCB"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QVq5FEnUz4s8"
      },
      "source": [
        "RCB_train = unite('/content/combined/RCB/train.jsonl', '/content/combined/RCB/val.jsonl')\r\n",
        "RCB_test = JSONL_handler('/content/combined/RCB/test.jsonl').to_pandas()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "raUj6PU0zSn_"
      },
      "source": [
        "steps_RCB = [('tfidf', TfidfVectorizer(analyzer='word', max_features=10000)),\r\n",
        "          ('func', FunctionTransformer(lambda x: x.todense(), accept_sparse=True)),\r\n",
        "         ('sgd', SGDClassifier(loss=\"log\", n_jobs=-1, alpha=0.00001, class_weight='balanced', random_state=42))]\r\n",
        "\r\n",
        "pipeline_RCB = Pipeline(steps_RCB)\r\n",
        "\r\n",
        "pipeline_RCB.fit(RCB_train.hypothesis, RCB_train.label)\r\n",
        "y_pred_RCB = pipeline_RCB.predict(RCB_test.hypothesis)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tsEEsUjf2Wtt"
      },
      "source": [
        "tfidf_rcb = Tfidf_Submisssion(RCB_test, y_pred_RCB, 'RCB')\r\n",
        "tfidf_rcb.test_output()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ChYnuLDqyAVY"
      },
      "source": [
        "### MuSeRC"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Da3ouaJQyI7v"
      },
      "source": [
        "RCB_train = unite('/content/combined/RCB/train.jsonl', '/content/combined/RCB/val.jsonl')\r\n",
        "RCB_test = JSONL_handler('/content/combined/RCB/test.jsonl').to_pandas()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HGfezPXDyKUB"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NkusSAPkyKwf"
      },
      "source": [
        "tfidf_rcb = Tfidf_Submisssion(RCB_test, y_pred_RCB, 'RCB')\r\n",
        "tfidf_rcb.test_output()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tt16TWzM5bR3"
      },
      "source": [
        "### TERRa"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pdr7HuLi56CD"
      },
      "source": [
        "TERRa_train = unite('/content/combined/TERRa/train.jsonl', '/content/combined/TERRa/val.jsonl')\r\n",
        "TERRa_test = JSONL_handler('/content/combined/TERRa/test.jsonl').to_pandas()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yi87kVdr5dxY"
      },
      "source": [
        "steps_TERRa = [('countvect', CountVectorizer(min_df=15, max_df=0.4, lowercase=True, analyzer ='char_wb', decode_error = 'ignore', ngram_range = (2, 4))),\r\n",
        "         ('sgd', SGDClassifier(alpha = 1e-08, loss=\"log\", n_jobs=-1, class_weight='balanced', random_state=42))]\r\n",
        "\r\n",
        "pipeline_TERRa = Pipeline(steps_TERRa)\r\n",
        "\r\n",
        "pipeline_TERRa.fit(TERRa_train.hypothesis, TERRa_train.label)\r\n",
        "y_pred_TERRa = pipeline_TERRa.predict(TERRa_test.hypothesis)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hgfqX79x5_zE"
      },
      "source": [
        "tfidf_terra = Tfidf_Submisssion(TERRa_test, y_pred_TERRa, 'TERRa')\r\n",
        "tfidf_terra.test_output()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LlsZGLAu6U1C"
      },
      "source": [
        "### DaNetQA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kjj-SXgl6XpT"
      },
      "source": [
        "DaNetQA_train = unite('/content/combined/DaNetQA/train.jsonl', '/content/combined/DaNetQA/val.jsonl')\r\n",
        "DaNetQA_test = JSONL_handler('/content/combined/DaNetQA/test.jsonl').to_pandas()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TJ90YD9a6aBY"
      },
      "source": [
        "steps_DaNetQA = [('vectorizer', TfidfVectorizer()),\r\n",
        "              ('sgd', SGDClassifier(loss=\"log\", n_jobs=-1, alpha=0.15, class_weight='balanced', random_state=42))]\r\n",
        "\r\n",
        "pipeline_DaNetQA = Pipeline(steps_DaNetQA)\r\n",
        "\r\n",
        "pipeline_DaNetQA.fit(DaNetQA_train.question, DaNetQA_train.label)\r\n",
        "y_pred_DaNetQA = pipeline_DaNetQA.predict(DaNetQA_test.question)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M3mG5sHj7TdO"
      },
      "source": [
        "tfidf_danetqa = Tfidf_Submisssion(DaNetQA_test, y_pred_DaNetQA, 'DaNetQa')\r\n",
        "tfidf_danetqa.test_output()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vp4MWtzPOIUq"
      },
      "source": [
        "### RUSSE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v4Z6EI5WOC_j"
      },
      "source": [
        "RUSSE_train = unite('/content/combined/RUSSE/train.jsonl', '/content/combined/RUSSE/val.jsonl')\r\n",
        "RUSSE_test = JSONL_handler('/content/combined/RUSSE/test.jsonl').to_pandas()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bgxB7dsrOSfc"
      },
      "source": [
        "def build_feature_RUSSE(row):\r\n",
        "    sentence1 = row[\"sentence1\"].strip()\r\n",
        "    sentence2 = row[\"sentence2\"].strip()\r\n",
        "    word = row[\"word\"].strip()\r\n",
        "    res = f\"{sentence1} {sentence2} {word}\"\r\n",
        "    return res\r\n",
        "\r\n",
        "train_concat = []\r\n",
        "for i, row in RUSSE_train.iterrows():\r\n",
        "    train_concat.append(build_feature_RUSSE(row))\r\n",
        "RUSSE_train['concatenated'] = train_concat\r\n",
        "\r\n",
        "valid_concat = []\r\n",
        "for i, row in RUSSE_test.iterrows():\r\n",
        "    valid_concat.append(build_feature_RUSSE(row))\r\n",
        "RUSSE_test['concatenated'] =  valid_concat"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1BGlbWXjOg9Y"
      },
      "source": [
        "steps_RUSSE = [('tfidf', TfidfVectorizer(analyzer = 'word', max_df = 0.6, min_df= 0.001, ngram_range =  (1,2))),\r\n",
        "         ('logreg', LogisticRegression(C = 1.01, class_weight='balanced'))]\r\n",
        "\r\n",
        "pipeline_RUSSE = Pipeline(steps_RUSSE)\r\n",
        "\r\n",
        "pipeline_RUSSE.fit(RUSSE_train.concatenated, RUSSE_train.label)\r\n",
        "y_pred_RUSSE = pipeline_RUSSE.predict(RUSSE_test.concatenated)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tn4d1QvzPJBQ"
      },
      "source": [
        "tfidf_russe = Tfidf_Submisssion(RUSSE_test, y_pred_RUSSE, 'RUSSE')\r\n",
        "tfidf_russe.test_output()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HmTCOY9tSLHd"
      },
      "source": [
        "### PARus"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v7QvfQzJSRYD"
      },
      "source": [
        "PARus_train = unite('/content/combined/PARus/train.jsonl', '/content/combined/PARus/val.jsonl')\r\n",
        "PARus_test = JSONL_handler('/content/combined/PARus/test.jsonl').to_pandas()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q3DMGV2OSKiM"
      },
      "source": [
        "def build_feature_PARus(row):\r\n",
        "    premise = str(row[\"premise\"]).strip()\r\n",
        "    choice1 = row[\"choice1\"]\r\n",
        "    choice2 = row[\"choice2\"]\r\n",
        "    label = row.get(\"label\")\r\n",
        "    question = \"Что было ПРИЧИНОЙ этого?\" if row[\"question\"] == \"cause\" else \"Что случилось в РЕЗУЛЬТАТЕ?\"\r\n",
        "    res = f\"{premise} {question} {choice1} {choice2}\"\r\n",
        "    return res\r\n",
        "\r\n",
        "\r\n",
        "train_concat = []\r\n",
        "for i, row in PARus_train.iterrows():\r\n",
        "    train_concat.append(build_feature_PARus(row))\r\n",
        "PARus_train['concatenated'] = train_concat\r\n",
        "\r\n",
        "valid_concat = []\r\n",
        "for i, row in PARus_test.iterrows():\r\n",
        "    valid_concat.append(build_feature_PARus(row))\r\n",
        "PARus_test['concatenated'] =  valid_concat"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X_5yCz-aSdPw"
      },
      "source": [
        "steps_PARus = [('tfidf', TfidfVectorizer(analyzer= 'word', max_df= 0.6, min_df= 0.04, ngram_range= (1, 2))),\r\n",
        "         ('logreg', LogisticRegression(C = 1e-10, class_weight='balanced'))]\r\n",
        "\r\n",
        "pipeline_PARus = Pipeline(steps_PARus)\r\n",
        "\r\n",
        "pipeline_PARus.fit(PARus_train.concatenated, PARus_train.label)\r\n",
        "y_pred_PARus = pipeline_PARus.predict(PARus_test.concatenated)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EeXsS7HKS94r"
      },
      "source": [
        "tfidf_parus = Tfidf_Submisssion(PARus_test, y_pred_PARus, 'PARus')\r\n",
        "tfidf_parus.test_output()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1nn08m3ATJMW"
      },
      "source": [
        "### LiDiRus"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cD2-ovnrTL60"
      },
      "source": [
        "LiDiRus_train = unite('/content/combined/TERRa/train.jsonl', '/content/combined/TERRa/val.jsonl').assign(merged=lambda x: x.premise + \"\\n\" + x.hypothesis)\r\n",
        "LiDiRus_test = JSONL_handler('/content/combined/LiDiRus/LiDiRus.jsonl').to_pandas().assign(merged=lambda x: x.sentence1 + \"\\n\" + x.sentence2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5AhpFcZUVMMF"
      },
      "source": [
        "steps_LiDiRus = [('tfidf', TfidfVectorizer(analyzer= 'char_wb', max_df= 0.6, min_df=0.091, ngram_range = (1, 1))),\r\n",
        "         ('logreg', LogisticRegression(C = 1.01, class_weight='balanced'))]\r\n",
        "\r\n",
        "pipeline_LiDiRus = Pipeline(steps_LiDiRus)\r\n",
        "\r\n",
        "pipeline_LiDiRus.fit(LiDiRus_train.merged, LiDiRus_train.label)\r\n",
        "y_pred_LiDiRus = pipeline_LiDiRus.predict(LiDiRus_test.merged)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oBGa0LaKWHar"
      },
      "source": [
        "tfidf_lidirus = Tfidf_Submisssion(LiDiRus_test, y_pred_LiDiRus, 'LiDiRus')\r\n",
        "tfidf_lidirus.test_output()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s4n1h3Kje-1O"
      },
      "source": [
        "# Heuristics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eudc_bvWfFOg"
      },
      "source": [
        "output_dir_heuristics_random = Path(\"heuristics_random_submission\")\r\n",
        "!mkdir $output_dir_heuristics_random"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ER5L92uRnmqJ"
      },
      "source": [
        "output_dir_heuristics_majority = Path(\"heuristics_majority_submission\")\r\n",
        "!mkdir $output_dir_heuristics_majority"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T-uURRZ1nrb5"
      },
      "source": [
        "output_dir_heuristics_rw = Path(\"heuristics_rw_submission\")\r\n",
        "!mkdir $output_dir_heuristics_rw"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vkrdG2dCfIOA"
      },
      "source": [
        "import json\r\n",
        "\r\n",
        "class Heuristic_submission():\r\n",
        "  def __init__(self, dataset, solver, path = None, path_valid = None, path_test = None):\r\n",
        "    self.dataset = dataset\r\n",
        "    self.path = '/content/combined/' + dataset + '/train.jsonl' if path is None else path\r\n",
        "    self.path_valid = '/content/combined/' + dataset + '/val.jsonl' if path_valid is None else path_valid\r\n",
        "    self.path_test = '/content/combined/' + dataset + '/test.jsonl' if path_test is None else path_test\r\n",
        "    self.solver = solver(path=self.path, path_valid= self.path_valid, path_test = self.path_test)\r\n",
        "\r\n",
        "  def test_output(self):\r\n",
        "    test = JSONL_handler(self.path_test).to_pandas()\r\n",
        "    test_pred = [{\"idx\": idx, \"label\": str(label).lower()} for idx, label in zip(test.idx, self.scores)]\r\n",
        "    return test_pred\r\n",
        "\r\n",
        "  def get_scores_random(self):\r\n",
        "    self.scores = self.solver.heuristics_all(final_decision=self.solver.random_choice)\r\n",
        "    filename = self.dataset + \".jsonl\"\r\n",
        "    self.save_output(self.test_output(), output_dir_heuristics_random / filename)\r\n",
        "  \r\n",
        "  def get_scores_majority(self):\r\n",
        "    self.scores = self.solver.heuristics_all(final_decision=self.solver.majority_class)\r\n",
        "    filename = self.dataset + \".jsonl\"\r\n",
        "    self.save_output(self.test_output(), output_dir_heuristics_majority / filename)\r\n",
        "\r\n",
        "  def get_scores_random_weighted(self):\r\n",
        "    self.scores = self.solver.heuristics_all(final_decision=self.solver.random_balanced_choice)\r\n",
        "    filename = self.dataset + \".jsonl\"\r\n",
        "    self.save_output(self.test_output(), output_dir_heuristics_rw / filename)\r\n",
        "  \r\n",
        "  def save_output(self, data, path):\r\n",
        "    with open(path, mode=\"w\") as file:\r\n",
        "        for line in sorted(data, key=lambda x: int(x.get(\"idx\"))):\r\n",
        "            line[\"idx\"] = int(line[\"idx\"])\r\n",
        "            file.write(f\"{json.dumps(line, ensure_ascii=False)}\\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JGYEsphefzxH"
      },
      "source": [
        "## Datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Ck9wn9ViCGQ"
      },
      "source": [
        "%%capture\r\n",
        "!pip install pymorphy2\r\n",
        "!pip install razdel\r\n",
        "!pip install natasha"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oHI10Lurf5L2"
      },
      "source": [
        "import pymorphy2\r\n",
        "import re\r\n",
        "from pymorphy2 import MorphAnalyzer\r\n",
        "import nltk\r\n",
        "from functools import lru_cache\r\n",
        "from base import BaseSolverSubmit\r\n",
        "from scipy import stats\r\n",
        "\r\n",
        "m = MorphAnalyzer()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-6Ag7Ivq8gcw"
      },
      "source": [
        "import numpy as np\r\n",
        "from natasha import (\r\n",
        "    Segmenter,\r\n",
        "    MorphVocab,  \r\n",
        "    NewsEmbedding,\r\n",
        "    NewsMorphTagger,\r\n",
        "    Doc\r\n",
        ")\r\n",
        "\r\n",
        "segmenter = Segmenter()\r\n",
        "morph_vocab = MorphVocab()\r\n",
        "emb = NewsEmbedding()\r\n",
        "morph_tagger = NewsMorphTagger(emb)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6whDZkEsf2Bb"
      },
      "source": [
        "### TERRa"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iZ-vKqCMf9Kx"
      },
      "source": [
        "class TERRaSolver(BaseSolverSubmit):\r\n",
        "   \r\n",
        "    def __init__(self, path: str, path_valid=None, path_test=None):\r\n",
        "        super(TERRaSolver, self).__init__(path, path_test, path_valid)\r\n",
        "   \r\n",
        "    def preprocess(self, columns):\r\n",
        "      for column in columns:\r\n",
        "        self.train[f\"{column}_lemmas\"] = self.train[column].apply(self.clean_text)\r\n",
        "        self.valid[f\"{column}_lemmas\"] = self.valid[column].apply(self.clean_text)\r\n",
        " \r\n",
        "    def words_only(self, text):\r\n",
        "      rg = re.compile(\"[А-Яа-яA-z]+\")\r\n",
        "      try:\r\n",
        "        return rg.findall(text.lower())\r\n",
        "      except:\r\n",
        "        return []\r\n",
        "\r\n",
        "    @lru_cache(maxsize=128)\r\n",
        "    def lemmatize_word(self, token, pymorphy=m):\r\n",
        "      return pymorphy.parse(token)[0].normal_form\r\n",
        "\r\n",
        "    def lemmatize_text(self, text):\r\n",
        "      return [self.lemmatize_word(w) for w in text]\r\n",
        "\r\n",
        "    def clean_text(self, text):\r\n",
        "      tokens = self.words_only(text)\r\n",
        "      lemmas = self.lemmatize_text(tokens)  \r\n",
        "      return lemmas\r\n",
        "    \r\n",
        "    def heuristics_all(self, final_decision=None):\r\n",
        "        y_pred = []\r\n",
        "        self.preprocess(columns=['premise', \"hypothesis\"])\r\n",
        "\r\n",
        "        for i, row in self.valid.iterrows():\r\n",
        "          \r\n",
        "          hyp = row.hypothesis.lower()\r\n",
        "          hyp_lem = set(row['hypothesis_lemmas'])\r\n",
        "          prem_lem = set(row['premise_lemmas'])\r\n",
        "          indic_non_ent = set(['только', 'мужчина'])\r\n",
        "\r\n",
        "          if hyp in row['premise'].lower():\r\n",
        "             y_pred.append('entailment')\r\n",
        "          elif len(prem_lem & hyp_lem)/len(hyp_lem) <= 1/3 or len(row['premise'].split()) < 29 or len(indic_non_ent & hyp_lem) > 0:\r\n",
        "            y_pred.append('not_entailment')\r\n",
        "          elif len(prem_lem & hyp_lem)/len(hyp_lem) == 0.75 or len(prem_lem & hyp_lem)/len(hyp_lem) == 1 or len(prem_lem & hyp_lem)/len(hyp_lem) == 2/3:\r\n",
        "            y_pred.append('entailment')\r\n",
        "          elif len(row['premise'].split()) > 32:\r\n",
        "            y_pred.append('entailment')\r\n",
        "          else:\r\n",
        "            y_pred.append(final_desicion(test_size=1)[0])\r\n",
        "        \r\n",
        "        return y_pred"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JNxAWwYxxlfZ"
      },
      "source": [
        "terra_heuristics = Heuristic_submission('TERRa', TERRaSolver)\r\n",
        "terra_heuristics.get_scores_random()\r\n",
        "terra_heuristics.get_scores_majority()\r\n",
        "terra_heuristics.get_scores_random_weighted()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cJSjaVPI62kX"
      },
      "source": [
        "### DaNetQA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dPU9TbPG66nu"
      },
      "source": [
        "class DaNetQASolver(BaseSolverSubmit):\r\n",
        "    \r\n",
        "    def __init__(self, path: str, path_valid=None, path_test=None):\r\n",
        "        super(DaNetQASolver, self).__init__(path, path_test, path_valid)\r\n",
        "   \r\n",
        "    def heuristics_all(self, final_decision=None):\r\n",
        "        y_pred = []\r\n",
        "\r\n",
        "        for i, row in self.valid.iterrows():\r\n",
        "\r\n",
        "            question = row.question.lower()\r\n",
        "            question_w_count = len(question.split())\r\n",
        "            passage_w_count = len(row.passage.split())\r\n",
        "\r\n",
        "            if re.search(\"был|(^есть)\", question):\r\n",
        "              y_pred.append(True)\r\n",
        "            elif re.search(\"^входит|едят|правда ли\", question):\r\n",
        "              y_pred.append(False)\r\n",
        "            elif question_w_count > 5:\r\n",
        "              y_pred.append(False)\r\n",
        "            elif passage_w_count >= 90:\r\n",
        "              y_pred.append(False)\r\n",
        "            else:\r\n",
        "              y_pred.append(final_desicion(test_size=1)[0])\r\n",
        "      \r\n",
        "        return y_pred"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qJrSBwrU7R6U"
      },
      "source": [
        "danetqa_heuristics = Heuristic_submission('DaNetQA', DaNetQASolver)\r\n",
        "danetqa_heuristics.get_scores_random()\r\n",
        "danetqa_heuristics.get_scores_majority()\r\n",
        "danetqa_heuristics.get_scores_random_weighted()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZFkkBWKb7s0_"
      },
      "source": [
        "### RCB"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hB_hgyyy7v9o"
      },
      "source": [
        "class RCBSolver(BaseSolverSubmit):\r\n",
        "\r\n",
        "    def __init__(self, path: str, path_valid=None, path_test=None):\r\n",
        "        super(RCBSolver, self).__init__(path, path_test, path_valid)\r\n",
        "   \r\n",
        "    def preprocess(self, columns):\r\n",
        "      for column in columns:\r\n",
        "        self.train[f\"{column}_lemmas\"] = self.train[column].apply(self.clean_text)\r\n",
        "        self.valid[f\"{column}_lemmas\"] = self.valid[column].apply(self.clean_text)\r\n",
        " \r\n",
        "    def words_only(self, text):\r\n",
        "      rg = re.compile(\"[А-Яа-яA-z]+\")\r\n",
        "      try:\r\n",
        "        return rg.findall(text.lower())\r\n",
        "      except:\r\n",
        "        return []\r\n",
        "\r\n",
        "    @lru_cache(maxsize=128)\r\n",
        "    def lemmatize_word(self, token, pymorphy=m):\r\n",
        "      return pymorphy.parse(token)[0].normal_form\r\n",
        "\r\n",
        "    def lemmatize_text(self, text):\r\n",
        "      return [self.lemmatize_word(w) for w in text]\r\n",
        "\r\n",
        "    def clean_text(self, text):\r\n",
        "      tokens = self.words_only(text)\r\n",
        "      lemmas = self.lemmatize_text(tokens)  \r\n",
        "      return lemmas\r\n",
        "    \r\n",
        "    def heuristics_all(self, final_decision=None):\r\n",
        "        y_pred = []\r\n",
        "        self.preprocess(columns=['premise', \"hypothesis\"])\r\n",
        "\r\n",
        "        for i, row in self.valid.iterrows():\r\n",
        "          \r\n",
        "          hyp = row.hypothesis.lower()\r\n",
        "          hyp_lem = set(row['hypothesis_lemmas'])\r\n",
        "          prem_lem = set(row['premise_lemmas'])\r\n",
        "          indic_neutral = set(['подозревать', 'cчитать', 'говорить', 'думать', 'надеяться', 'понять', 'уверять'])\r\n",
        "          indic_ent = set(['признать'])\r\n",
        "\r\n",
        "          if hyp in row['premise'].lower() or len(indic_ent & prem_lem) > 0 :\r\n",
        "             y_pred.append('entailment')\r\n",
        "          elif len(prem_lem & hyp_lem)/len(hyp_lem) == 0.75:\r\n",
        "            y_pred.append('entailment')\r\n",
        "          elif len(indic_neutral & prem_lem) > 0:\r\n",
        "            y_pred.append('neutral')\r\n",
        "          elif len(row.hypothesis.split()) < 5:\r\n",
        "            y_pred.append('contradiction')\r\n",
        "          elif len(row.hypothesis.split()) >= 5 and len(row.hypothesis.split()) <= 7:\r\n",
        "            y_pred.append('neutral')\r\n",
        "          elif len(row['premise'].split()) > 30:\r\n",
        "            y_pred.append('entailment')\r\n",
        "          else:\r\n",
        "            y_pred.append(final_desicion(test_size=1)[0])\r\n",
        "        \r\n",
        "        return y_pred"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VHR8UdQR7wUj"
      },
      "source": [
        "rcb_heuristics = Heuristic_submission('RCB', RCBSolver)\r\n",
        "rcb_heuristics.get_scores_random()\r\n",
        "rcb_heuristics.get_scores_majority()\r\n",
        "rcb_heuristics.get_scores_random_weighted()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B8UVEo188Sz6"
      },
      "source": [
        "### PARus"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0PygoCcV8ZUc"
      },
      "source": [
        "class ParusSolver(BaseSolverSubmit):\r\n",
        "   \r\n",
        "    def __init__(self, path: str, path_valid=None, path_test=None):\r\n",
        "        super(ParusSolver, self).__init__(path, path_test, path_valid)\r\n",
        "   \r\n",
        "    def preprocess(self, columns):\r\n",
        "\r\n",
        "        for column in columns:\r\n",
        "            self.train[f\"{column}_lemmas\"] = self.train[column].apply(self.lemmatize)\r\n",
        "            self.valid[f\"{column}_lemmas\"] = self.valid[column].apply(self.lemmatize)\r\n",
        "\r\n",
        "    def lemmatize(self, text):\r\n",
        "        \"\"\"\r\n",
        "        param text: str\r\n",
        "        return: List of lemmas (strings)\r\n",
        "        \"\"\"\r\n",
        "\r\n",
        "        doc = Doc(text)\r\n",
        "        doc.segment(segmenter)\r\n",
        "        doc.tag_morph(morph_tagger)\r\n",
        "\r\n",
        "        for token in doc.tokens:\r\n",
        "            token.lemmatize(morph_vocab)\r\n",
        "        lemmas = [token.lemma for token in doc.tokens]\r\n",
        "        return lemmas\r\n",
        "\r\n",
        "    \r\n",
        "    def heuristics_all(self, final_decision=None):\r\n",
        "        \"\"\"\r\n",
        "        This heruistic chooses the option that has more common lemmas with premise\r\n",
        "        If the amount of common words is equal for both choices, it uses {final_desicion}\r\n",
        "        function (one of BaseSolver functions) to predict\r\n",
        "        param: final_decision (function)\r\n",
        "        \"\"\"\r\n",
        "        y_pred = []\r\n",
        "        self.preprocess(columns=['premise', 'choice1', 'choice2'])\r\n",
        "\r\n",
        "        for i, row in self.valid.iterrows():\r\n",
        "            words1 = set(row.choice1_lemmas)\r\n",
        "            words2 = set(row.choice2_lemmas)\r\n",
        "            premise = set(row.premise_lemmas)\r\n",
        "            overlap1 = len(premise & words1)\r\n",
        "            overlap2 = len(premise & words2)\r\n",
        "            if overlap1 > overlap2:\r\n",
        "                y_pred.append(0)\r\n",
        "            elif overlap2 > overlap1:\r\n",
        "                y_pred.append(1)\r\n",
        "            else:\r\n",
        "                y_pred.append(final_desicion(test_size=1)[0])\r\n",
        "        return y_pred"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BY95BGQm80DP"
      },
      "source": [
        "parus_heuristics = Heuristic_submission('PARus', ParusSolver)\r\n",
        "parus_heuristics.get_scores_random()\r\n",
        "parus_heuristics.get_scores_majority()\r\n",
        "parus_heuristics.get_scores_random_weighted()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PZWlrntBdNz3"
      },
      "source": [
        "### RUSSE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GbHYMb8DdNY_"
      },
      "source": [
        "class RusseSolver(BaseSolverSubmit):\r\n",
        "\r\n",
        "    def __init__(self, path: str, path_valid=None, path_test=None):\r\n",
        "        super(RusseSolver, self).__init__(path, path_test, path_valid)\r\n",
        "\r\n",
        "    def heuristics_all(self, final_decision=None):\r\n",
        "        y_pred = []\r\n",
        "\r\n",
        "        for i, row in self.valid.iterrows():\r\n",
        "            tokens1 = set(row.sentence1.split())\r\n",
        "            tokens2 = set(row.sentence2.split())\r\n",
        "\r\n",
        "            if len(tokens1 & tokens2) / len(tokens1 | tokens2) > 0.10:\r\n",
        "                y_pred.append(True)\r\n",
        "            else:\r\n",
        "                options = np.array([final_decision(test_size=1)[0] for i in range(0,3)])\r\n",
        "                y_pred.append(stats.mode(options)[0][0])\r\n",
        "        return y_pred"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7_OJeqwCqu3s"
      },
      "source": [
        "russe_heuristics = Heuristic_submission('RUSSE', RusseSolver)\r\n",
        "russe_heuristics.get_scores_random()\r\n",
        "russe_heuristics.get_scores_majority()\r\n",
        "russe_heuristics.get_scores_random_weighted()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wq1eLrv8dwye"
      },
      "source": [
        "# Make submission file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S16wnAe7eBMq"
      },
      "source": [
        "!7z a \"random_submission.zip\" $output_dir\r\n",
        "!7z a \"majority_submission.zip\" $output_dir_majority\r\n",
        "!7z a \"random_weighted_submission.zip\" $output_dir_random_weighted"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xCnSTIme7v8K"
      },
      "source": [
        "!7z a \"random_weighted_submission.zip\" $output_dir_tfidf"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}