{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BaseLine.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "MqAs4c1oc7Gw",
        "Pc-gFqkFgxVH",
        "ZKPswiVygsd2",
        "blR1K9jMhXE-",
        "Q86ieCFyl8OG",
        "QOm7jooUmUgA"
      ],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tatiana-iazykova/2020_HACK_RUSSIANSUPERGLUE/blob/main/BaseLine.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QnhjR-EFgY2c"
      },
      "source": [
        "import os\r\n",
        "import random \r\n",
        "import warnings\r\n",
        "warnings.filterwarnings(\"ignore\")\r\n",
        "\r\n",
        "import numpy as np\r\n",
        "import pandas as pd\r\n",
        "from sklearn.metrics import classification_report, accuracy_score, f1_score\r\n",
        "from sklearn.metrics import matthews_corrcoef\r\n",
        "\r\n",
        "\r\n",
        "def seed_everything(seed: int):\r\n",
        "    random.seed(seed)\r\n",
        "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\r\n",
        "    np.random.RandomState(seed)\r\n",
        "    np.random.seed(seed)\r\n",
        "    # torch.manual_seed(seed)\r\n",
        "    # torch.cuda.manual_seed(seed)\r\n",
        "    # torch.backends.cudnn.deterministic = True\r\n",
        "\r\n",
        "seed_everything(42)\r\n",
        "SEEDS = [42, 23, 1234567]"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5euwTaKdWidr"
      },
      "source": [
        "class Baseline:\r\n",
        "\r\n",
        "    def __init__(self, path: str, path_valid=None, seed=42):\r\n",
        "        \r\n",
        "        self.path = path\r\n",
        "        self.train = pd.read_json(path_or_buf=path, lines=True)\r\n",
        "        self.seed = seed\r\n",
        "        if path_valid:\r\n",
        "            self.valid = pd.read_json(path_or_buf=path_valid, lines=True)\r\n",
        "        else:\r\n",
        "            self.valid = None\r\n",
        "    \r\n",
        "    def f1_accuracy_only(self, predict_function):\r\n",
        "        \"\"\"\r\n",
        "        F1-macro and accuracy metrics only\r\n",
        "        \"\"\"\r\n",
        "        test_size = len(self.valid)\r\n",
        "        y_true = list(self.valid.label)\r\n",
        "        y_pred = predict_function(test_size=test_size)\r\n",
        "        accuracy = accuracy_score(y_true, y_pred)\r\n",
        "        f1_macro = f1_score(y_true, y_pred, average='macro')\r\n",
        "        return f1_macro, accuracy    \r\n",
        "\r\n",
        "    def mc_only(self, predict_function):\r\n",
        "        \"\"\"\r\n",
        "        Mathews Correlation only (for Lidirus)\r\n",
        "        \"\"\"\r\n",
        "        test_size = len(self.train)\r\n",
        "        y_true = self.train.label\r\n",
        "        y_pred = predict_function(test_size=test_size)\r\n",
        "        mc_metric = matthews_corrcoef(y_true, y_pred)\r\n",
        "        return mc_metric\r\n",
        "    \r\n",
        "    def majority(self):\r\n",
        "        \"\"\"\r\n",
        "        Majority prediction and classification report/mc metric\r\n",
        "        \"\"\"\r\n",
        "        if self.valid is not None:\r\n",
        "            test_size = len(self.valid)\r\n",
        "            y_true = list(self.valid.label)\r\n",
        "        else:\r\n",
        "            print(\"No Valid dataset. Making Predictions for Train dataset\")\r\n",
        "            test_size = len(self.train)\r\n",
        "            y_true = self.train.label\r\n",
        "            \r\n",
        "        print(f\"\\nMaking Prediction based on Majority Class\")\r\n",
        "        y_pred = self.majority_class(test_size=test_size)\r\n",
        "\r\n",
        "        if 'lidirus' in self.path.lower():\r\n",
        "            print(f\" Matthews Correlation: {self.show_mc(y_true, y_pred)}\")\r\n",
        "        else:\r\n",
        "            self.show_report(y_true, y_pred)\r\n",
        "\r\n",
        "    def show_report(self, y_true, y_pred):\r\n",
        "        print(classification_report(y_true, y_pred))\r\n",
        "\r\n",
        "    def show_mc(self, y_true, y_pred):\r\n",
        "        return matthews_corrcoef(y_true, y_pred)\r\n",
        "       \r\n",
        "    def majority_class(self, test_size):\r\n",
        "        \"\"\"\r\n",
        "        Make prediction based on majority class of train dataset\r\n",
        "        test_size: how many predictions should be made\r\n",
        "        return: List of predictions\r\n",
        "        \"\"\"\r\n",
        "\r\n",
        "        prediction = self.train.label.mode()[0]\r\n",
        "        y_pred = [prediction] * test_size\r\n",
        "        return y_pred\r\n",
        "\r\n",
        "    def random_choice(self, test_size):\r\n",
        "        \"\"\"\r\n",
        "        Make random predictions\r\n",
        "        label: label column in df (str)\r\n",
        "        test_size: how many predictions should be made\r\n",
        "        return: List of predictions\r\n",
        "        \"\"\"\r\n",
        "        options = sorted(self.train.label.unique())\r\n",
        "        if test_size != 1:\r\n",
        "            np.random.seed(self.seed)\r\n",
        "        y_pred = np.random.choice(options, size=test_size)\r\n",
        "        return y_pred\r\n",
        "\r\n",
        "    def random_balanced_choice(self, test_size):\r\n",
        "        \"\"\"\r\n",
        "        Make random predictions with calculated probabilities\r\n",
        "        label: label column in df (str)\r\n",
        "        test_size: how many predictions should be made\r\n",
        "        return: List of predictions\r\n",
        "        \"\"\"\r\n",
        "        frequences = dict(self.train.label.value_counts(normalize=True))\r\n",
        "\r\n",
        "        labels = []\r\n",
        "        probs = []\r\n",
        "        for key, value in frequences.items():\r\n",
        "            labels.append(key)\r\n",
        "            probs.append(value)\r\n",
        "        if test_size != 1:\r\n",
        "            np.random.seed(self.seed)\r\n",
        "        y_pred = np.random.choice(labels, size=test_size, p=probs)\r\n",
        "        return y_pred"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MqAs4c1oc7Gw"
      },
      "source": [
        "# LiDiRus"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bENHZbtgcU1X"
      },
      "source": [
        "%%capture\r\n",
        "%%bash\r\n",
        "# change url if you want to work with a different RSG dataset\r\n",
        "wget -q \"https://russiansuperglue.com/ru/tasks/download/LiDiRus\" -O temp.zip\r\n",
        "unzip temp.zip -d data\r\n",
        "\r\n",
        "# remove unnecessary directories and files\r\n",
        "rm temp.zip\r\n",
        "rm -r data/__MACOSX\r\n",
        "rm -r sample_data/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ny4uU3RIdmaa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d05f8b2e-5499-48c9-936f-3aa90db6488e"
      },
      "source": [
        "lidirus = Baseline(path='data/LiDiRus/LiDiRus.jsonl')\r\n",
        "lidirus.majority()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "No Valid dataset. Making Predictions for Train dataset\n",
            "\n",
            "Making Prediction based on Majority Class\n",
            " Matthews Correlation: 0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "te30U5VkY4v1",
        "outputId": "94a8e470-39d6-4847-b2d2-9ee7b3e61f10"
      },
      "source": [
        "metrics = []\r\n",
        "for seed in SEEDS:\r\n",
        "    lidirus = Baseline(path='data/LiDiRus/LiDiRus.jsonl', seed=seed)\r\n",
        "    metrics.append(lidirus.mc_only(predict_function=lidirus.random_choice))\r\n",
        "print(f\"Random Choice\")\r\n",
        "print(f\"Average MC score over {len(SEEDS)} experiments: {np.array(metrics).mean()}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Random Choice\n",
            "Average MC score over 3 experiments: 0.019522405921979163\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0suqqOYQaQh6",
        "outputId": "2b4eab3e-9186-4775-fbfd-1379473aba99"
      },
      "source": [
        "metrics = []\r\n",
        "for seed in SEEDS:\r\n",
        "    lidirus = Baseline(path='data/LiDiRus/LiDiRus.jsonl', seed=seed)\r\n",
        "    metrics.append(lidirus.mc_only(predict_function=lidirus.random_balanced_choice))\r\n",
        "print(f\"Random Balanced Choice\")\r\n",
        "print(f\"Average MC score over {len(SEEDS)} experiments: {np.array(metrics).mean()}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Random Balanced Choice\n",
            "Average MC score over 3 experiments: -0.012200643650637838\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pc-gFqkFgxVH"
      },
      "source": [
        "# RCB"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "40gI92kog9cE"
      },
      "source": [
        "%%capture\r\n",
        "%%bash\r\n",
        "# change url if you want to work with a different RSG dataset\r\n",
        "wget -q \"https://russiansuperglue.com/tasks/download/RCB\" -O temp.zip\r\n",
        "unzip temp.zip -d data\r\n",
        "\r\n",
        "# remove unnecessary directories and files\r\n",
        "rm temp.zip\r\n",
        "rm -r data/__MACOSX\r\n",
        "rm -r sample_data/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uuY5uonIhRpw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1f869bcd-f450-4108-aee3-87c450d0d2f5"
      },
      "source": [
        "rcb = Baseline(path='data/RCB/train.jsonl',\r\n",
        "               path_valid='data/RCB/val.jsonl')\r\n",
        "rcb.majority()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Making Prediction based on Majority Class\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "contradiction       0.00      0.00      0.00        30\n",
            "   entailment       0.00      0.00      0.00        74\n",
            "      neutral       0.53      1.00      0.69       116\n",
            "\n",
            "     accuracy                           0.53       220\n",
            "    macro avg       0.18      0.33      0.23       220\n",
            " weighted avg       0.28      0.53      0.36       220\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BZ_MC73ibSEw",
        "outputId": "c2d88c40-08bd-443f-b691-0b76a94ca8c1"
      },
      "source": [
        "f1a_metrics = []\r\n",
        "acc_metrics = []\r\n",
        "for seed in SEEDS:\r\n",
        "    rcb = Baseline(path='data/RCB/train.jsonl', \r\n",
        "                   path_valid='data/RCB/val.jsonl',\r\n",
        "                   seed=seed)\r\n",
        "    \r\n",
        "    f1, acc = rcb.f1_accuracy_only(predict_function=rcb.random_choice)\r\n",
        "    f1a_metrics.append(f1)\r\n",
        "    acc_metrics.append(acc)\r\n",
        "print(f\"Random Choice\")\r\n",
        "print(f\"Average Accuracy score over {len(SEEDS)} experiments: {np.array(acc_metrics).mean()}\")\r\n",
        "print(f\"Average F1-macro score over {len(SEEDS)} experiments: {np.array(f1a_metrics).mean()}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Random Choice\n",
            "Average Accuracy score over 3 experiments: 0.35000000000000003\n",
            "Average F1-macro score over 3 experiments: 0.3184430925012964\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OPxhAhEtcTsx",
        "outputId": "6ae5e826-48e8-4501-84db-aa149e2f063d"
      },
      "source": [
        "f1a_metrics = []\r\n",
        "acc_metrics = []\r\n",
        "for seed in SEEDS:\r\n",
        "    rcb = Baseline(path='data/RCB/train.jsonl', \r\n",
        "                   path_valid='data/RCB/val.jsonl',\r\n",
        "                   seed=seed)\r\n",
        "    \r\n",
        "    f1, acc = rcb.f1_accuracy_only(predict_function=rcb.random_balanced_choice)\r\n",
        "    f1a_metrics.append(f1)\r\n",
        "    acc_metrics.append(acc)\r\n",
        "print(f\"Random Balanced Choice\")\r\n",
        "print(f\"Average Accuracy score over {len(SEEDS)} experiments: {np.array(acc_metrics).mean()}\")\r\n",
        "print(f\"Average F1-macro score over {len(SEEDS)} experiments: {np.array(f1a_metrics).mean()}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Random Balanced Choice\n",
            "Average Accuracy score over 3 experiments: 0.37121212121212116\n",
            "Average F1-macro score over 3 experiments: 0.3076621045848644\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZKPswiVygsd2"
      },
      "source": [
        "# PARus"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GmADdzyagPnS"
      },
      "source": [
        "%%capture\r\n",
        "%%bash\r\n",
        "# change url if you want to work with a different RSG dataset\r\n",
        "wget -q  \"https://russiansuperglue.com/tasks/download/PARus\" -O temp.zip\r\n",
        "unzip temp.zip -d data\r\n",
        "\r\n",
        "# remove unnecessary directories and files\r\n",
        "rm temp.zip\r\n",
        "rm -r data/__MACOSX\r\n",
        "rm -r sample_data/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XlSkJxHegzxR",
        "outputId": "405c68b6-82fc-43ae-be9b-062cd17afa0c"
      },
      "source": [
        "parus = Baseline(path='data/PARus/train.jsonl',\r\n",
        "               path_valid='data/PARus/val.jsonl')\r\n",
        "parus.majority()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Making Prediction based on Majority Class\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00        55\n",
            "           1       0.45      1.00      0.62        45\n",
            "\n",
            "    accuracy                           0.45       100\n",
            "   macro avg       0.23      0.50      0.31       100\n",
            "weighted avg       0.20      0.45      0.28       100\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MRafiprZcj0k",
        "outputId": "3dafafa6-ab1e-4d72-82fb-2f8f0ec79dd5"
      },
      "source": [
        "f1a_metrics = []\r\n",
        "acc_metrics = []\r\n",
        "for seed in SEEDS:\r\n",
        "    parus = Baseline(path='data/PARus/train.jsonl',\r\n",
        "                     path_valid='data/PARus/val.jsonl',\r\n",
        "                     seed=seed)\r\n",
        "    \r\n",
        "    f1, acc = parus.f1_accuracy_only(predict_function=parus.random_choice)\r\n",
        "    f1a_metrics.append(f1)\r\n",
        "    acc_metrics.append(acc)\r\n",
        "print(f\"Random Choice\")\r\n",
        "print(f\"Average Accuracy score over {len(SEEDS)} experiments: {np.array(acc_metrics).mean()}\")\r\n",
        "print(f\"Average F1-macro score over {len(SEEDS)} experiments: {np.array(f1a_metrics).mean()}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Random Choice\n",
            "Average Accuracy score over 3 experiments: 0.5166666666666666\n",
            "Average F1-macro score over 3 experiments: 0.5151684790713317\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P4mpKhqjct58",
        "outputId": "3ef546e6-34d0-4cbd-9560-e53a64efa7ba"
      },
      "source": [
        "f1a_metrics = []\r\n",
        "acc_metrics = []\r\n",
        "for seed in SEEDS:\r\n",
        "    parus = Baseline(path='data/PARus/train.jsonl',\r\n",
        "                     path_valid='data/PARus/val.jsonl',\r\n",
        "                     seed=seed)\r\n",
        "    \r\n",
        "    f1, acc = parus.f1_accuracy_only(predict_function=parus.random_balanced_choice)\r\n",
        "    f1a_metrics.append(f1)\r\n",
        "    acc_metrics.append(acc)\r\n",
        "print(f\"Random Balanced Choice\")\r\n",
        "print(f\"Average Accuracy score over {len(SEEDS)} experiments: {np.array(acc_metrics).mean()}\")\r\n",
        "print(f\"Average F1-macro score over {len(SEEDS)} experiments: {np.array(f1a_metrics).mean()}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Random Balanced Choice\n",
            "Average Accuracy score over 3 experiments: 0.52\n",
            "Average F1-macro score over 3 experiments: 0.5195209881342803\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "blR1K9jMhXE-"
      },
      "source": [
        "#TERRa"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i6kdosACg4Ts"
      },
      "source": [
        "%%capture\r\n",
        "%%bash\r\n",
        "# change url if you want to work with a different RSG dataset\r\n",
        "wget -q  \"https://russiansuperglue.com/tasks/download/TERRa\" -O temp.zip\r\n",
        "unzip temp.zip -d data\r\n",
        "\r\n",
        "# remove unnecessary directories and files\r\n",
        "rm temp.zip\r\n",
        "rm -r data/__MACOSX\r\n",
        "rm -r sample_data/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "28ijrayzhhYy",
        "outputId": "c18dedca-d89a-476d-f131-79bd20f78aeb"
      },
      "source": [
        "terra = Baseline(path='data/TERRa/train.jsonl',\r\n",
        "               path_valid='data/TERRa/val.jsonl')\r\n",
        "terra.majority()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Making Prediction based on Majority Class\n",
            "                precision    recall  f1-score   support\n",
            "\n",
            "    entailment       0.50      1.00      0.67       153\n",
            "not_entailment       0.00      0.00      0.00       154\n",
            "\n",
            "      accuracy                           0.50       307\n",
            "     macro avg       0.25      0.50      0.33       307\n",
            "  weighted avg       0.25      0.50      0.33       307\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "flZuvClBdFds",
        "outputId": "53dde5db-4f6c-4737-ded5-2c98d47d2846"
      },
      "source": [
        "f1a_metrics = []\r\n",
        "acc_metrics = []\r\n",
        "for seed in SEEDS:\r\n",
        "    terra = Baseline(path='data/TERRa/train.jsonl',\r\n",
        "                     path_valid='data/TERRa/val.jsonl',\r\n",
        "                     seed=seed)\r\n",
        "    \r\n",
        "    f1, acc = terra.f1_accuracy_only(predict_function=terra.random_choice)\r\n",
        "    f1a_metrics.append(f1)\r\n",
        "    acc_metrics.append(acc)\r\n",
        "print(f\"Random Choice\")\r\n",
        "print(f\"Average Accuracy score over {len(SEEDS)} experiments: {np.array(acc_metrics).mean()}\")\r\n",
        "print(f\"Average F1-macro score over {len(SEEDS)} experiments: {np.array(f1a_metrics).mean()}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Random Choice\n",
            "Average Accuracy score over 3 experiments: 0.498371335504886\n",
            "Average F1-macro score over 3 experiments: 0.4981518746617879\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WPIr2wgbdF7W",
        "outputId": "96628490-f708-4b15-c67d-f5d7cf055975"
      },
      "source": [
        "f1a_metrics = []\r\n",
        "acc_metrics = []\r\n",
        "for seed in SEEDS:\r\n",
        "    terra = Baseline(path='data/TERRa/train.jsonl',\r\n",
        "                     path_valid='data/TERRa/val.jsonl',\r\n",
        "                     seed=seed)\r\n",
        "    \r\n",
        "    f1, acc = terra.f1_accuracy_only(predict_function=terra.random_balanced_choice)\r\n",
        "    f1a_metrics.append(f1)\r\n",
        "    acc_metrics.append(acc)\r\n",
        "print(f\"Random Balanced Choice\")\r\n",
        "print(f\"Average Accuracy score over {len(SEEDS)} experiments: {np.array(acc_metrics).mean()}\")\r\n",
        "print(f\"Average F1-macro score over {len(SEEDS)} experiments: {np.array(f1a_metrics).mean()}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Random Balanced Choice\n",
            "Average Accuracy score over 3 experiments: 0.49294245385450597\n",
            "Average F1-macro score over 3 experiments: 0.49242854242830375\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q86ieCFyl8OG"
      },
      "source": [
        "# RUSSE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IkOWAo0ChvmJ"
      },
      "source": [
        "%%capture\r\n",
        "%%bash\r\n",
        "# change url if you want to work with a different RSG dataset\r\n",
        "wget -q \"https://russiansuperglue.com/tasks/download/RUSSE\" -O temp.zip\r\n",
        "unzip temp.zip -d data\r\n",
        "\r\n",
        "# remove unnecessary directories and files\r\n",
        "rm temp.zip\r\n",
        "rm -r data/__MACOSX\r\n",
        "rm -r sample_data/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IV_BkMz0mEH5",
        "outputId": "4b9f7064-36d4-46d3-8574-8410102aeb68"
      },
      "source": [
        "russe = Baseline(path='data/RUSSE/train.jsonl',\r\n",
        "               path_valid='data/RUSSE/val.jsonl')\r\n",
        "russe.majority()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Making Prediction based on Majority Class\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       False       0.63      1.00      0.77      5366\n",
            "        True       0.00      0.00      0.00      3139\n",
            "\n",
            "    accuracy                           0.63      8505\n",
            "   macro avg       0.32      0.50      0.39      8505\n",
            "weighted avg       0.40      0.63      0.49      8505\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DmbOv3VZeT-V",
        "outputId": "c4d9f903-2e52-4e92-f4f7-5775a16ee413"
      },
      "source": [
        "f1a_metrics = []\r\n",
        "acc_metrics = []\r\n",
        "for seed in SEEDS:\r\n",
        "    russe = Baseline(path='data/RUSSE/train.jsonl',\r\n",
        "               path_valid='data/RUSSE/val.jsonl',\r\n",
        "               seed=seed)\r\n",
        "    \r\n",
        "    f1, acc = russe.f1_accuracy_only(predict_function=russe.random_choice)\r\n",
        "    f1a_metrics.append(f1)\r\n",
        "    acc_metrics.append(acc)\r\n",
        "print(f\"Random Choice\")\r\n",
        "print(f\"Average Accuracy score over {len(SEEDS)} experiments: {np.array(acc_metrics).mean()}\")\r\n",
        "print(f\"Average F1-macro score over {len(SEEDS)} experiments: {np.array(f1a_metrics).mean()}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Random Choice\n",
            "Average Accuracy score over 3 experiments: 0.49900058788947677\n",
            "Average F1-macro score over 3 experiments: 0.49057702373389867\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FOTZhlJPem1z",
        "outputId": "1fea796a-2bdf-4c67-d876-d136ea855987"
      },
      "source": [
        "f1a_metrics = []\r\n",
        "acc_metrics = []\r\n",
        "for seed in SEEDS:\r\n",
        "    russe = Baseline(path='data/RUSSE/train.jsonl',\r\n",
        "               path_valid='data/RUSSE/val.jsonl',\r\n",
        "               seed=seed)\r\n",
        "    \r\n",
        "    f1, acc = russe.f1_accuracy_only(predict_function=russe.random_balanced_choice)\r\n",
        "    f1a_metrics.append(f1)\r\n",
        "    acc_metrics.append(acc)\r\n",
        "print(f\"Random Balanced Choice\")\r\n",
        "print(f\"Average Accuracy score over {len(SEEDS)} experiments: {np.array(acc_metrics).mean()}\")\r\n",
        "print(f\"Average F1-macro score over {len(SEEDS)} experiments: {np.array(f1a_metrics).mean()}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Random Balanced Choice\n",
            "Average Accuracy score over 3 experiments: 0.5342347638643935\n",
            "Average F1-macro score over 3 experiments: 0.4962201497224709\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QOm7jooUmUgA"
      },
      "source": [
        "# RWSD"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QZEvW6V0mJNs"
      },
      "source": [
        "%%capture\r\n",
        "%%bash\r\n",
        "# change url if you want to work with a different RSG dataset\r\n",
        "wget -q \"https://russiansuperglue.com/tasks/download/RWSD\" -O temp.zip\r\n",
        "unzip temp.zip -d data\r\n",
        "\r\n",
        "# remove unnecessary directories and files\r\n",
        "rm temp.zip\r\n",
        "rm -r data/__MACOSX\r\n",
        "rm -r sample_data/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4quyCGu_mZJ-",
        "outputId": "14dc5356-e5d2-41cf-c4f8-00b4df4f6567"
      },
      "source": [
        "rwsd = Baseline(path='data/RWSD/train.jsonl',\r\n",
        "               path_valid='data/RWSD/val.jsonl')\r\n",
        "rwsd.majority()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Making Prediction based on Majority Class\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       False       0.55      1.00      0.71       113\n",
            "        True       0.00      0.00      0.00        91\n",
            "\n",
            "    accuracy                           0.55       204\n",
            "   macro avg       0.28      0.50      0.36       204\n",
            "weighted avg       0.31      0.55      0.39       204\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2l_7AxFre0ev",
        "outputId": "b555e195-48f7-4b29-fd32-bcb164710df4"
      },
      "source": [
        "f1a_metrics = []\r\n",
        "acc_metrics = []\r\n",
        "for seed in SEEDS:\r\n",
        "    rwsd = Baseline(path='data/RWSD/train.jsonl',\r\n",
        "                    path_valid='data/RWSD/val.jsonl',\r\n",
        "                    seed=seed)\r\n",
        "    \r\n",
        "    f1, acc = rwsd.f1_accuracy_only(predict_function=rwsd.random_choice)\r\n",
        "    f1a_metrics.append(f1)\r\n",
        "    acc_metrics.append(acc)\r\n",
        "print(f\"Random Choice\")\r\n",
        "print(f\"Average Accuracy score over {len(SEEDS)} experiments: {np.array(acc_metrics).mean()}\")\r\n",
        "print(f\"Average F1-macro score over {len(SEEDS)} experiments: {np.array(f1a_metrics).mean()}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Random Choice\n",
            "Average Accuracy score over 3 experiments: 0.5261437908496732\n",
            "Average F1-macro score over 3 experiments: 0.5238139284486554\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vfAY4vMHfYxt",
        "outputId": "c8b1a306-96cd-4b46-9c5c-308859b017e3"
      },
      "source": [
        "f1a_metrics = []\r\n",
        "acc_metrics = []\r\n",
        "for seed in SEEDS:\r\n",
        "    rwsd = Baseline(path='data/RWSD/train.jsonl',\r\n",
        "                    path_valid='data/RWSD/val.jsonl',\r\n",
        "                    seed=seed)\r\n",
        "    \r\n",
        "    f1, acc = rwsd.f1_accuracy_only(predict_function=rwsd.random_balanced_choice)\r\n",
        "    f1a_metrics.append(f1)\r\n",
        "    acc_metrics.append(acc)\r\n",
        "print(f\"Random Balanced Choice\")\r\n",
        "print(f\"Average Accuracy score over {len(SEEDS)} experiments: {np.array(acc_metrics).mean()}\")\r\n",
        "print(f\"Average F1-macro score over {len(SEEDS)} experiments: {np.array(f1a_metrics).mean()}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Random Balanced Choice\n",
            "Average Accuracy score over 3 experiments: 0.4950980392156863\n",
            "Average F1-macro score over 3 experiments: 0.4908715414536949\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9apAB6uQmn0Z"
      },
      "source": [
        "# DaNetQA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rq79JBJomdL7"
      },
      "source": [
        "%%capture\r\n",
        "%%bash\r\n",
        "# change url if you want to work with a different RSG dataset\r\n",
        "wget -q \"https://russiansuperglue.com/tasks/download/DaNetQA\" -O temp.zip\r\n",
        "unzip temp.zip -d data\r\n",
        "\r\n",
        "# remove unnecessary directories and files\r\n",
        "rm temp.zip\r\n",
        "rm -r data/__MACOSX\r\n",
        "rm -r sample_data/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8btsn1ZQmu0A",
        "outputId": "50f6c749-1789-43a3-f5a2-b702678ac072"
      },
      "source": [
        "danetqa = Baseline(path='data/DaNetQA/train.jsonl',\r\n",
        "               path_valid='data/DaNetQA/val.jsonl')\r\n",
        "danetqa.majority()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Making Prediction based on Majority Class\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       False       0.00      0.00      0.00       409\n",
            "        True       0.50      1.00      0.67       412\n",
            "\n",
            "    accuracy                           0.50       821\n",
            "   macro avg       0.25      0.50      0.33       821\n",
            "weighted avg       0.25      0.50      0.34       821\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IOkvxRw4fha0",
        "outputId": "1bb323b3-1bce-40e4-d8e9-26e8c8d4981b"
      },
      "source": [
        "f1a_metrics = []\r\n",
        "acc_metrics = []\r\n",
        "for seed in SEEDS:\r\n",
        "    danetqa = Baseline(path='data/DaNetQA/train.jsonl',\r\n",
        "                       path_valid='data/DaNetQA/val.jsonl',\r\n",
        "                       seed=seed)\r\n",
        "    \r\n",
        "    f1, acc = danetqa.f1_accuracy_only(predict_function=danetqa.random_choice)\r\n",
        "    f1a_metrics.append(f1)\r\n",
        "    acc_metrics.append(acc)\r\n",
        "print(f\"Random Choice\")\r\n",
        "print(f\"Average Accuracy score over {len(SEEDS)} experiments: {np.array(acc_metrics).mean()}\")\r\n",
        "print(f\"Average F1-macro score over {len(SEEDS)} experiments: {np.array(f1a_metrics).mean()}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Random Choice\n",
            "Average Accuracy score over 3 experiments: 0.5002030044660982\n",
            "Average F1-macro score over 3 experiments: 0.5000465922796313\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JgSSYPyGfuHd",
        "outputId": "30e2ddcc-dd08-4d10-9f32-c2e86fedf8b1"
      },
      "source": [
        "f1a_metrics = []\r\n",
        "acc_metrics = []\r\n",
        "for seed in SEEDS:\r\n",
        "    danetqa = Baseline(path='data/DaNetQA/train.jsonl',\r\n",
        "                       path_valid='data/DaNetQA/val.jsonl',\r\n",
        "                       seed=seed)\r\n",
        "    \r\n",
        "    f1, acc = danetqa.f1_accuracy_only(predict_function=danetqa.random_balanced_choice)\r\n",
        "    f1a_metrics.append(f1)\r\n",
        "    acc_metrics.append(acc)\r\n",
        "print(f\"Random Balanced Choice\")\r\n",
        "print(f\"Average Accuracy score over {len(SEEDS)} experiments: {np.array(acc_metrics).mean()}\")\r\n",
        "print(f\"Average F1-macro score over {len(SEEDS)} experiments: {np.array(f1a_metrics).mean()}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Random Balanced Choice\n",
            "Average Accuracy score over 3 experiments: 0.4973609419407227\n",
            "Average F1-macro score over 3 experiments: 0.48881400729269947\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MpqLRgRYS-XQ"
      },
      "source": [
        "# MuSeRC"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iylDLzE8TFXw"
      },
      "source": [
        "%%capture\r\n",
        "%%bash\r\n",
        "# change url if you want to work with a different RSG dataset\r\n",
        "wget -q \"https://russiansuperglue.com/tasks/download/MuSeRC\" -O temp.zip\r\n",
        "unzip temp.zip -d data\r\n",
        "\r\n",
        "# remove unnecessary directories and files\r\n",
        "rm temp.zip\r\n",
        "rm -r data/__MACOSX\r\n",
        "rm -r sample_data/"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cBciHtbnbOu7"
      },
      "source": [
        "%%capture\r\n",
        "%%bash\r\n",
        "wget -q \"https://github.com/RussianNLP/RussianSuperGLUE/raw/master/tfidf_baseline/MuSeRC.py\" -O MuSeRC.py"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NbTAOK2UrXMQ"
      },
      "source": [
        "import functools\r\n",
        "import jsonlines\r\n",
        "import numpy as np\r\n",
        "from MuSeRC import MuSeRCMetrics, MuSeRC_metrics\r\n",
        "\r\n",
        "def MuSeRC_metrics(pred, labels):\r\n",
        "    metrics = MuSeRCMetrics()\r\n",
        "    em = metrics.exact_match_simple(pred, labels)\r\n",
        "    em0 = metrics.exact_match_metrics_origin(pred, labels, 0)\r\n",
        "    f1 = metrics.per_dataset_metric(pred, labels)\r\n",
        "    f1a = f1[-1]\r\n",
        "    return em0, f1a\r\n",
        "\r\n",
        "Measures = MuSeRCMetrics\r\n",
        "\r\n",
        "def eval_MuSeRC(train_path, val_path, test_path, vect):\r\n",
        "    test_score, test_pred = eval_part_MuSeRC(test_path, vect)\r\n",
        "    return None, {\r\n",
        "        \"train\": eval_part_MuSeRC(train_path, vect)[0],\r\n",
        "        \"val\": eval_part_MuSeRC(val_path, vect)[0],\r\n",
        "        \"test\": test_score,\r\n",
        "        \"test_pred\": test_pred\r\n",
        "    }\r\n",
        "\r\n",
        "def get_stats_MuSeRC(path, vect):\r\n",
        "    with jsonlines.open(path) as reader:\r\n",
        "        lines = list(reader)\r\n",
        "    labels = []\r\n",
        "    for row in lines:\r\n",
        "        _, _labels, __ = get_row_pred_MuSeRC(row, vect)\r\n",
        "        labels.extend(_labels)\r\n",
        "    labels = [item for sublist in labels for item in sublist]\r\n",
        "    MAJOR_LABEL = max(labels, key=labels.count)\r\n",
        "    labels = pd.Series(labels)\r\n",
        "    frequences = dict(labels.value_counts(normalize=True))\r\n",
        "    OPTIONS = []\r\n",
        "    PROBS = []\r\n",
        "    for key, value in frequences.items():\r\n",
        "        OPTIONS.append(key)\r\n",
        "        PROBS.append(value)\r\n",
        "    return MAJOR_LABEL, OPTIONS, PROBS\r\n",
        "    \r\n",
        "\r\n",
        "def eval_part_MuSeRC(path, vect):\r\n",
        "    with jsonlines.open(path) as reader:\r\n",
        "        lines = list(reader)\r\n",
        "    preds = []\r\n",
        "    labels = []\r\n",
        "    res = []\r\n",
        "    for row in lines:\r\n",
        "        pred, lbls, res_ids = get_row_pred_MuSeRC(row, vect)\r\n",
        "        preds.extend(pred)\r\n",
        "        labels.extend(lbls)\r\n",
        "        res.append(res_ids)\r\n",
        "    return MuSeRC_metrics(preds, labels), res\r\n",
        "\r\n",
        "\r\n",
        "def get_row_pred_MuSeRC(row, vect):\r\n",
        "    res = []\r\n",
        "    labels = []\r\n",
        "    res_ids = {\"idx\": row[\"idx\"], \"passage\": {\"questions\": []}}\r\n",
        "    for line in row[\"passage\"][\"questions\"]:\r\n",
        "        res_line = {\"idx\": line[\"idx\"], \"answers\": []}\r\n",
        "        line_answers = []\r\n",
        "        line_labels = []\r\n",
        "        for answ in line[\"answers\"]:\r\n",
        "            line_labels.append(answ.get(\"label\", 0))\r\n",
        "            answ = f\"{line['question']} {answ['text']}\"\r\n",
        "            line_answers.append(answ)\r\n",
        "\r\n",
        "        if MODE == 'MAJOR':\r\n",
        "            pred  = [0] * len(line[\"answers\"])\r\n",
        "        elif MODE == 'RANDOM':\r\n",
        "            pred = list(np.random.choice(OPTIONS, size=len(line['answers'])))\r\n",
        "        elif MODE == 'RB':\r\n",
        "            pred = list(np.random.choice(OPTIONS, size=len(line['answers']), p=PROBS))\r\n",
        "        else:\r\n",
        "            size = np.random.choice(np.arange(1, 5), size=1)[0]\r\n",
        "            if size > len(line_answers):\r\n",
        "                pred = [1] * len(line_answers)\r\n",
        "            else:\r\n",
        "                pred = np.random.choice(np.arange(len(line_answers)),\r\n",
        "                                        size=size,\r\n",
        "                                        replace=False)\r\n",
        "            pred = [int(idx in pred) for idx in range(len(line[\"answers\"]))]\r\n",
        "        res.append(pred)\r\n",
        "        labels.append(line_labels)\r\n",
        "        for answ, p in zip(line[\"answers\"], pred):\r\n",
        "            res_line[\"answers\"].append({\"idx\": answ[\"idx\"], \"label\": p})\r\n",
        "        res_ids[\"passage\"][\"questions\"].append(res_line)\r\n",
        "    return res, labels, res_ids\r\n",
        "\r\n",
        "train_path = \"data/MuSeRC/train.jsonl\"\r\n",
        "val_path = \"data/MuSeRC/val.jsonl\"\r\n",
        "test_path = \"data/MuSeRC/test.jsonl\""
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j_Fy07_7atPk",
        "outputId": "f2b49304-3cd2-487f-9e78-47a3c0056f5e"
      },
      "source": [
        "MODE = 'MAJOR'\r\n",
        "MAJOR_LABEL, OPTIONS, PROBS = get_stats_MuSeRC(train_path, 'No vect')\r\n",
        "MAJOR_LABEL, OPTIONS, PROBS"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0, [0, 1], [0.5496234309623431, 0.4503765690376569])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "flnLKlEoh87e",
        "outputId": "503451fc-4c43-483c-8036-2ac56c2eb660"
      },
      "source": [
        "MODE = 'MAJOR'\r\n",
        "em_metrics = []\r\n",
        "f1a_metrics = []\r\n",
        "\r\n",
        "for i in range(3):\r\n",
        "    _, MuSeRC_scores = eval_MuSeRC(train_path, val_path, test_path, 'No vect')\r\n",
        "    em = MuSeRC_scores[\"val\"][0]\r\n",
        "    f1a = MuSeRC_scores[\"val\"][1]\r\n",
        "    em_metrics.append(em)\r\n",
        "    f1a_metrics.append(f1a)\r\n",
        "\r\n",
        "print(f\"Major Class\")\r\n",
        "print(f\"Average EM score over 3 experiments: {np.array(em_metrics).mean()}\")\r\n",
        "print(f\"Average F1-a score over 3 experiments: {np.array(f1a_metrics).mean()}\")"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Major Class\n",
            "Average EM score over 3 experiments: 0.0\n",
            "Average F1-a score over 3 experiments: 0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lRPQBaCdjvO4",
        "outputId": "3609a2fe-5987-4b02-c425-dc5984b8b087"
      },
      "source": [
        "MODE = 'RANDOM'\r\n",
        "em_metrics = []\r\n",
        "f1a_metrics = []\r\n",
        "\r\n",
        "for i in range(3):\r\n",
        "    _, MuSeRC_scores = eval_MuSeRC(train_path, val_path, test_path, 'No vect')\r\n",
        "    em = MuSeRC_scores[\"val\"][0]\r\n",
        "    f1a = MuSeRC_scores[\"val\"][1]\r\n",
        "    em_metrics.append(em)\r\n",
        "    f1a_metrics.append(f1a)\r\n",
        "\r\n",
        "print(f\"Random Choice\")\r\n",
        "print(f\"Average EM score over 3 experiments: {np.array(em_metrics).mean()}\")\r\n",
        "print(f\"Average F1-a score over 3 experiments: {np.array(f1a_metrics).mean()}\")"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Random Choice\n",
            "Average EM score over 3 experiments: 0.06238185255198488\n",
            "Average F1-a score over 3 experiments: 0.4690999401921761\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WxToKtFzkmvx",
        "outputId": "0d84478e-6f97-4479-a0f9-34709a5d77cf"
      },
      "source": [
        "MODE = 'RB'\r\n",
        "em_metrics = []\r\n",
        "f1a_metrics = []\r\n",
        "\r\n",
        "for i in range(3):\r\n",
        "    _, MuSeRC_scores = eval_MuSeRC(train_path, val_path, test_path, 'No vect')\r\n",
        "    em = MuSeRC_scores[\"val\"][0]\r\n",
        "    f1a = MuSeRC_scores[\"val\"][1]\r\n",
        "    em_metrics.append(em)\r\n",
        "    f1a_metrics.append(f1a)\r\n",
        "\r\n",
        "print(f\"Random Balanced Choice\")\r\n",
        "print(f\"Average EM score over 3 experiments: {np.array(em_metrics).mean()}\")\r\n",
        "print(f\"Average F1-a score over 3 experiments: {np.array(f1a_metrics).mean()}\")"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Random Balanced Choice\n",
            "Average EM score over 3 experiments: 0.074354127284184\n",
            "Average F1-a score over 3 experiments: 0.4376949078560884\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SldUfFCGk46W",
        "outputId": "2a946822-90ee-4190-da71-d709a29e59ca"
      },
      "source": [
        "MODE = 'RANDOM_OLD'\r\n",
        "em_metrics = []\r\n",
        "f1a_metrics = []\r\n",
        "\r\n",
        "for i in range(3):\r\n",
        "    _, MuSeRC_scores = eval_MuSeRC(train_path, val_path, test_path, 'No vect')\r\n",
        "    em = MuSeRC_scores[\"val\"][0]\r\n",
        "    f1a = MuSeRC_scores[\"val\"][1]\r\n",
        "    em_metrics.append(em)\r\n",
        "    f1a_metrics.append(f1a)\r\n",
        "\r\n",
        "print(f\"Random Choice\")\r\n",
        "print(f\"Average EM score over 3 experiments: {np.array(em_metrics).mean()}\")\r\n",
        "print(f\"Average F1-a score over 3 experiments: {np.array(f1a_metrics).mean()}\")"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Random Choice\n",
            "Average EM score over 3 experiments: 0.07939508506616257\n",
            "Average F1-a score over 3 experiments: 0.4998549551984078\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VdEhwR5RQpL3"
      },
      "source": [
        "# RuCoS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LHrwszlYSm-r"
      },
      "source": [
        "%%capture\r\n",
        "%%bash\r\n",
        "# change url if you want to work with a different RSG dataset\r\n",
        "wget -q \"https://russiansuperglue.com/tasks/download/RuCoS\" -O temp.zip\r\n",
        "unzip temp.zip -d data\r\n",
        "\r\n",
        "# remove unnecessary directories and files\r\n",
        "rm temp.zip\r\n",
        "rm -r data/__MACOSX\r\n",
        "rm -r sample_data/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7k0B4-zcSrZQ"
      },
      "source": [
        "%%capture\r\n",
        "%%bash\r\n",
        "wget -q \"https://github.com/RussianNLP/RussianSuperGLUE/raw/master/tfidf_baseline/RuCoS.py\" -O RuCoS.py"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jbOr2zRPSyW_"
      },
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\r\n",
        "import jsonlines\r\n",
        "import numpy as np\r\n",
        "from collections import Counter\r\n",
        "import string\r\n",
        "import re\r\n",
        "import sys\r\n",
        "import numpy as np\r\n",
        "from RuCoS import normalize_answer, f1_score, exact_match_score, metric_max_over_ground_truths\r\n",
        "\r\n",
        "\r\n",
        "def evaluate(dataset, predictions):\r\n",
        "    f1 = exact_match = total = 0\r\n",
        "    correct_ids = []\r\n",
        "    for prediction, passage in zip(predictions, dataset):\r\n",
        "        prediction = prediction[\"label\"]\r\n",
        "        for qa in passage['qas']:\r\n",
        "            total += 1\r\n",
        "            ground_truths = list(map(lambda x: x['text'], qa.get(\"answers\", \"\")))\r\n",
        "\r\n",
        "            _exact_match = metric_max_over_ground_truths(exact_match_score, prediction, ground_truths)\r\n",
        "            if int(_exact_match) == 1:\r\n",
        "                correct_ids.append(qa['idx'])\r\n",
        "            exact_match += _exact_match\r\n",
        "\r\n",
        "            f1 += metric_max_over_ground_truths(f1_score, prediction, ground_truths)\r\n",
        "\r\n",
        "    exact_match = exact_match / total\r\n",
        "    f1 = f1 / total\r\n",
        "    return exact_match, f1\r\n",
        "\r\n",
        "\r\n",
        "def eval_RuCoS(train_path, val_path, test_path, vect):\r\n",
        "    test_score, test_pred = eval_part(test_path, vect)\r\n",
        "    return None, {\r\n",
        "        \"train\": eval_part(train_path, vect)[0],\r\n",
        "        \"val\": eval_part(val_path, vect)[0],\r\n",
        "        \"test\": test_score,\r\n",
        "        \"test_pred\": test_pred\r\n",
        "    }\r\n",
        "\r\n",
        "\r\n",
        "def eval_part(path, vect):\r\n",
        "    with jsonlines.open(path) as reader:\r\n",
        "        lines = list(reader)\r\n",
        "    preds = []\r\n",
        "    for row in lines:\r\n",
        "        pred = get_row_pred(row, vect)\r\n",
        "        preds.append({\r\n",
        "            \"idx\": row[\"idx\"],\r\n",
        "            \"label\": pred\r\n",
        "        })\r\n",
        "    return evaluate(lines, preds), preds\r\n",
        "\r\n",
        "\r\n",
        "def get_row_pred(row, vect):\r\n",
        "    res = []\r\n",
        "    words = [\r\n",
        "        row[\"passage\"][\"text\"][x[\"start\"]: x[\"end\"]]\r\n",
        "        for x in row[\"passage\"][\"entities\"]]\r\n",
        "    for line in row[\"qas\"]:\r\n",
        "        line_candidates = []\r\n",
        "        for word in words:\r\n",
        "            line_candidates.append(line[\"query\"].replace(\"@placeholder\", word))\r\n",
        "        \r\n",
        "        \r\n",
        "        pred_idx = np.random.choice(np.arange(1, len(line_candidates)),\r\n",
        "                                    size=1)[0]\r\n",
        "        pred = np.array(words)[pred_idx]      \r\n",
        "        res.append(pred)\r\n",
        "    return \" \".join(res)\r\n",
        "\r\n",
        "train_path = \"data/RuCoS/train.jsonl\"\r\n",
        "val_path = \"data/RuCoS/val.jsonl\"\r\n",
        "test_path = \"data/RuCoS/test.jsonl\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NZEmfIB0iwJB",
        "outputId": "d6c2ec5b-62c6-4bac-9696-907e0a3407b9"
      },
      "source": [
        "em_metrics = []\r\n",
        "f1_metrics = []\r\n",
        "\r\n",
        "for i in range(3):\r\n",
        "    _, RuCoS_scores = eval_RuCoS(train_path, val_path, test_path, 'No vect')\r\n",
        "    em = RuCoS_scores['val'][0]\r\n",
        "    f1 = RuCoS_scores['val'][1]\r\n",
        "    em_metrics.append(em)\r\n",
        "    f1_metrics.append(f1)\r\n",
        "\r\n",
        "print(f\"Random Choice\")\r\n",
        "print(f\"Average EM score over 3 experiments: {np.array(em_metrics).mean()}\")\r\n",
        "print(f\"Average F1 score over 3 experiments: {np.array(f1_metrics).mean()}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Random Choice\n",
            "Average EM score over 3 experiments: 0.23267784083410323\n",
            "Average F1 score over 3 experiments: 0.23464871761031195\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}